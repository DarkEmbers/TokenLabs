{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3cd2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b04049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee1228b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove pull requests and remove issues with no comments\n",
    "issues_dataset = issues_dataset.filter(lambda x: x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87e56922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'body', 'html_url', 'comments'],\n",
      "    num_rows: 808\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = issues_dataset.to_pandas()\n",
    "df = df[[\"title\", \"body\", \"html_url\", \"comments\"]]\n",
    "issues_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "print(issues_dataset)\n",
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7cc615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "html_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comments",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "24ffd72a-3b45-4734-bcdb-926794c836d9",
       "rows": [
        [
         "0",
         "Protect master branch",
         "After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\r\n- 00cc036fea7c7745cfe722360036ed306796a3f2\r\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\r\n- ...\r\n\r\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\r\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\r\n  - Currently, simple merge commits are already disabled\r\n  - I propose to disable rebase merging as well\r\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\r\n  - ~~This protection would reject direct pushes to master branch~~\r\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\r\n- [x] Protect the master branch only from direct pushing of **merge commits**\r\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\r\n  - No need to disable/re-enable this protection on each release \r\n\r\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.",
         "https://github.com/huggingface/datasets/issues/2945",
         "Cool, I think we can do both :)"
        ],
        [
         "1",
         "Protect master branch",
         "After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:\r\n- 00cc036fea7c7745cfe722360036ed306796a3f2\r\n- 13ae8c98602bbad8197de3b9b425f4c78f582af1\r\n- ...\r\n\r\nI propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:\r\n- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch\r\n  - Currently, simple merge commits are already disabled\r\n  - I propose to disable rebase merging as well\r\n- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~\r\n  - ~~This protection would reject direct pushes to master branch~~\r\n  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~\r\n- [x] Protect the master branch only from direct pushing of **merge commits**\r\n  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).\r\n  - No need to disable/re-enable this protection on each release \r\n\r\nThis purpose of this Issue is to open a discussion about this problem and to agree in a solution.",
         "https://github.com/huggingface/datasets/issues/2945",
         "@lhoestq now the 2 are implemented.\r\n\r\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history)."
        ],
        [
         "2",
         "Backwards compatibility broken for cached datasets that use `.filter()`",
         "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2943",
         "Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\r\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?"
        ],
        [
         "3",
         "Backwards compatibility broken for cached datasets that use `.filter()`",
         "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2943",
         "If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests."
        ],
        [
         "4",
         "Backwards compatibility broken for cached datasets that use `.filter()`",
         "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2943",
         "Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR"
        ],
        [
         "5",
         "Backwards compatibility broken for cached datasets that use `.filter()`",
         "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2943",
         "I just merged a fix, let me know if you're still having this kind of issues :)\r\n\r\nWe'll do a release soon to make this fix available"
        ],
        [
         "6",
         "Backwards compatibility broken for cached datasets that use `.filter()`",
         "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2943",
         "Definitely works on several manual cases with our dummy datasets, thank you @lhoestq !"
        ],
        [
         "7",
         "Backwards compatibility broken for cached datasets that use `.filter()`",
         "## Describe the bug\r\nAfter upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with \r\n`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`\r\n\r\nRelated feature: https://github.com/huggingface/datasets/pull/2836\r\n\r\n:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) \r\n\r\n## Workaround\r\nRemove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.\r\n\r\n## Steps to reproduce the bug\r\n1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.\r\n\r\n2. `pip install datasets==1.11.0` and run the following snippet:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nids = [\"1272-141231-0000\"]\r\nds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\nds = ds.filter(lambda x: x[\"id\"] in ids)\r\n```\r\n3. `pip install datasets==1.12.1` and re-run the code again\r\n\r\n## Expected results\r\nSame result as with the previous `datasets` version.\r\n\r\n## Actual results\r\n```bash\r\nReusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)\r\nLoading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow\r\nTraceback (most recent call last):\r\n  File \"./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py\", line 5, in <module>\r\n    ds = ds.filter(lambda x: x[\"id\"] in ids)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2169, in filter\r\n    indices = self.map(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1686, in map\r\n    return self._map_single(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 185, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py\", line 398, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1896, in _map_single\r\n    return Dataset.from_file(cache_file_name, info=info, split=self.split)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 343, in from_file\r\n    return cls(\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 282, in __init__\r\n    self.info.features = self.info.features.reorder_fields_as(inferred_features)\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1151, in reorder_fields_as\r\n    return Features(recursive_reorder(self, other))\r\n  File \"./envs/transformers/lib/python3.8/site-packages/datasets/features.py\", line 1140, in recursive_reorder\r\n    raise ValueError(f\"Keys mismatch: between {source} and {target}\" + stack_position)\r\nValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2943",
         "Fixed by #2947."
        ],
        [
         "8",
         "OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError",
         "## Describe the bug\r\n\r\nCannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.\r\n\r\n## Steps to reproduce the bug\r\n\r\n```python\r\n>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]\r\n```\r\n\r\n## Expected results\r\n\r\nLoading is successful.\r\n\r\n## Actual results\r\n\r\nLoading throws above error.\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2941",
         "I tried `unshuffled_original_da` and it is also not working"
        ],
        [
         "9",
         "load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied",
         "## Describe the bug\r\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('wiki_bio')\r\n```\r\n\r\n## Expected results\r\nIt is expected that the dataset downloads without any errors.\r\n\r\n## Actual results\r\nPermissionError see trace below:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\contextlib.py\", line 120, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 598, in incomplete_dir\r\n    os.rename(tmp_dir, dirname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'\r\n```\r\nBy commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.\r\n\r\nIt seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.22449-SP0\r\n- Python version: 3.8.12\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2937",
         "Hi @daqieq, thanks for reporting.\r\n\r\nUnfortunately, I was not able to reproduce this bug:\r\n```ipython\r\nIn [1]: from datasets import load_dataset\r\n   ...: ds = load_dataset('wiki_bio')\r\nDownloading: 7.58kB [00:00, 26.3kB/s]\r\nDownloading: 2.71kB [00:00, ?B/s]\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\r\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nDownloading: 334MB [01:17, 4.32MB/s]\r\nDataset wiki_bio downloaded and prepared to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9. Subsequent calls will reuse thi\r\ns data.\r\n```\r\n\r\nThis kind of error messages usually happen because:\r\n- Your running Python script hasn't write access to that directory\r\n- You have another program (the File Explorer?) already browsing inside that directory"
        ],
        [
         "10",
         "load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied",
         "## Describe the bug\r\nStandard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('wiki_bio')\r\n```\r\n\r\n## Expected results\r\nIt is expected that the dataset downloads without any errors.\r\n\r\n## Actual results\r\nPermissionError see trace below:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\contextlib.py\", line 120, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\username\\.conda\\envs\\hf\\lib\\site-packages\\datasets\\builder.py\", line 598, in incomplete_dir\r\n    os.rename(tmp_dir, dirname)\r\nPermissionError: [WinError 5] Access is denied: 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\\\Users\\\\username\\\\.cache\\\\huggingface\\\\datasets\\\\wiki_bio\\\\default\\\\1.1.0\\\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'\r\n```\r\nBy commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.\r\n\r\nIt seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.\r\n\r\n## Environment info\r\n- `datasets` version: 1.12.1\r\n- Platform: Windows-10-10.0.22449-SP0\r\n- Python version: 3.8.12\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2937",
         "Thanks @albertvillanova for looking at it! I tried on my personal Windows machine and it downloaded just fine.\r\n\r\nRunning on my work machine and on a colleague's machine it is consistently hitting this error. It's not a write access issue because the `.incomplete` directory is written just fine. It just won't rename and then it deletes the directory in the `finally` step. Also the zip file is written and extracted fine in the downloads directory.\r\n\r\nThat leaves another program that might be interfering, and there are plenty of those in my work machine ... (full antivirus, data loss prevention, etc.). So the question remains, why not extend the `try` block to allow catching the error and circle back to the rename after the unknown program is finished doing its 'stuff'. This is the approach that I read about in the linked repo (see my comments above).\r\n\r\nIf it's not high priority, that's fine. However, if someone were to write an PR that solved this issue in our environment in an `except` clause, would it be reviewed for inclusion in a future release? Just wondering whether I should spend any more time on this issue."
        ],
        [
         "11",
         "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows",
         "To reproduce:\r\n```python\r\nimport datasets as ds\r\nimport weakref\r\nimport gc\r\n\r\nd = ds.load_dataset(\"mnist\", split=\"train\")\r\nref = weakref.ref(d._data.table)\r\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\r\ndel tfd, d\r\ngc.collect()\r\nassert ref() is None, \"Error: there is at least one reference left\"\r\n```\r\n\r\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\r\n\r\nMoreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.\r\n\r\ncc @Rocketknight1 ",
         "https://github.com/huggingface/datasets/issues/2934",
         "I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `tf.python.data.ops.dataset_ops` is a valid path for TF 1.x) that deletes the linked dataset, which is assigned to the dataset object as a property. Will open a draft PR soon!"
        ],
        [
         "12",
         "to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows",
         "To reproduce:\r\n```python\r\nimport datasets as ds\r\nimport weakref\r\nimport gc\r\n\r\nd = ds.load_dataset(\"mnist\", split=\"train\")\r\nref = weakref.ref(d._data.table)\r\ntfd = d.to_tf_dataset(\"image\", batch_size=1, shuffle=False, label_cols=\"label\")\r\ndel tfd, d\r\ngc.collect()\r\nassert ref() is None, \"Error: there is at least one reference left\"\r\n```\r\n\r\nThis causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.\r\n\r\nMoreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.\r\n\r\ncc @Rocketknight1 ",
         "https://github.com/huggingface/datasets/issues/2934",
         "Thanks a lot for investigating !"
        ],
        [
         "13",
         "Conda build fails",
         "## Describe the bug\r\nCurrent `datasets` version in conda is 1.9 instead of 1.12.\r\n\r\nThe build of the conda package fails.\r\n",
         "https://github.com/huggingface/datasets/issues/2932",
         "Why 1.9 ?\r\n\r\nhttps://anaconda.org/HuggingFace/datasets currently says 1.11"
        ],
        [
         "14",
         "Conda build fails",
         "## Describe the bug\r\nCurrent `datasets` version in conda is 1.9 instead of 1.12.\r\n\r\nThe build of the conda package fails.\r\n",
         "https://github.com/huggingface/datasets/issues/2932",
         "Alright I added 1.12.0 and 1.12.1 and fixed the conda build #2952 "
        ],
        [
         "15",
         "Mutable columns argument breaks set_format",
         "## Describe the bug\r\nIf you pass a mutable list to the `columns` argument of `set_format` and then change the list afterwards, the returned columns also change.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"glue\", \"cola\")\r\n\r\ncolumn_list = [\"idx\", \"label\"]\r\ndataset.set_format(\"python\", columns=column_list)\r\ncolumn_list[1] = \"foo\"  # Change the list after we call `set_format`\r\ndataset['train'][:4].keys()\r\n```\r\n\r\n## Expected results\r\n```python\r\ndict_keys(['idx', 'label'])\r\n```\r\n\r\n## Actual results\r\n```python\r\ndict_keys(['idx'])\r\n```",
         "https://github.com/huggingface/datasets/issues/2930",
         "Pushed a fix to my branch #2731 "
        ],
        [
         "16",
         "Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument",
         "## Describe the bug\r\nUpgrading to 1.12 caused `dataset.filter` call to fail with \r\n\r\n> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels\r\n\r\n\r\n## Steps to reproduce the bug\r\n```pythondef \r\n\r\nfilter_good_rows(\r\n    ex: Dict,\r\n    valid_rel_labels: Set[str],\r\n    valid_ner_labels: Set[str],\r\n    tokenizer: PreTrainedTokenizerFast,\r\n) -> bool:\r\n    \"\"\"Get the good rows\"\"\"\r\n    encoding = get_encoding_for_text(text=ex[\"text\"], tokenizer=tokenizer)\r\n    ex[\"encoding\"] = encoding\r\n    for relation in ex[\"relations\"]:\r\n        if not is_valid_relation(relation, valid_rel_labels):\r\n            return False\r\n    for span in ex[\"spans\"]:\r\n        if not is_valid_span(span, valid_ner_labels, encoding):\r\n            return False\r\n    return True\r\n    \r\ndef get_dataset():    \r\n    loader_path = str(Path(__file__).parent / \"prodigy_dataset_builder.py\")\r\n    ds = load_dataset(\r\n        loader_path,\r\n        name=\"prodigy-dataset\",\r\n        data_files=sorted(file_paths),\r\n        cache_dir=cache_dir,\r\n    )[\"train\"]\r\n\r\n    valid_ner_labels = set(vocab.ner_category)\r\n    valid_relations = set(vocab.relation_types.keys())\r\n    ds = ds.filter(\r\n        filter_good_rows,\r\n        fn_kwargs=dict(\r\n            valid_rel_labels=valid_relations,\r\n            valid_ner_labels=valid_ner_labels,\r\n            tokenizer=vocab.tokenizer,\r\n        ),\r\n        keep_in_memory=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n```\r\n\r\n`ds` is a `DatasetDict` produced by a jsonl dataset.\r\nThis runs fine on 1.11 but fails on 1.12\r\n\r\n**Stack Trace**\r\n\r\n\r\n\r\n## Expected results\r\n\r\nI expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11\r\n\r\n## Actual results\r\n```\r\ntf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl\r\n    ds = ds.filter(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter\r\n    indices = self.map(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map\r\n    return self._map_single(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ninputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}\r\nindices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0\r\n\r\n    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):\r\n        \"\"\"Utility to apply the function on a selection of columns.\"\"\"\r\n        nonlocal update_data\r\n        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n        if offset == 0:\r\n            effective_indices = indices\r\n        else:\r\n            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n        processed_inputs = (\r\n>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n        )\r\nE       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'\r\n\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Mac\r\n- Python version: 3.8.9\r\n- PyArrow version: pyarrow==5.0.0\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/2927",
         "Thanks for reporting, I'm looking into it :)"
        ],
        [
         "17",
         "Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument",
         "## Describe the bug\r\nUpgrading to 1.12 caused `dataset.filter` call to fail with \r\n\r\n> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels\r\n\r\n\r\n## Steps to reproduce the bug\r\n```pythondef \r\n\r\nfilter_good_rows(\r\n    ex: Dict,\r\n    valid_rel_labels: Set[str],\r\n    valid_ner_labels: Set[str],\r\n    tokenizer: PreTrainedTokenizerFast,\r\n) -> bool:\r\n    \"\"\"Get the good rows\"\"\"\r\n    encoding = get_encoding_for_text(text=ex[\"text\"], tokenizer=tokenizer)\r\n    ex[\"encoding\"] = encoding\r\n    for relation in ex[\"relations\"]:\r\n        if not is_valid_relation(relation, valid_rel_labels):\r\n            return False\r\n    for span in ex[\"spans\"]:\r\n        if not is_valid_span(span, valid_ner_labels, encoding):\r\n            return False\r\n    return True\r\n    \r\ndef get_dataset():    \r\n    loader_path = str(Path(__file__).parent / \"prodigy_dataset_builder.py\")\r\n    ds = load_dataset(\r\n        loader_path,\r\n        name=\"prodigy-dataset\",\r\n        data_files=sorted(file_paths),\r\n        cache_dir=cache_dir,\r\n    )[\"train\"]\r\n\r\n    valid_ner_labels = set(vocab.ner_category)\r\n    valid_relations = set(vocab.relation_types.keys())\r\n    ds = ds.filter(\r\n        filter_good_rows,\r\n        fn_kwargs=dict(\r\n            valid_rel_labels=valid_relations,\r\n            valid_ner_labels=valid_ner_labels,\r\n            tokenizer=vocab.tokenizer,\r\n        ),\r\n        keep_in_memory=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n```\r\n\r\n`ds` is a `DatasetDict` produced by a jsonl dataset.\r\nThis runs fine on 1.11 but fails on 1.12\r\n\r\n**Stack Trace**\r\n\r\n\r\n\r\n## Expected results\r\n\r\nI expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11\r\n\r\n## Actual results\r\n```\r\ntf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl\r\n    ds = ds.filter(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter\r\n    indices = self.map(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map\r\n    return self._map_single(\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper\r\n    out = func(self, *args, **kwargs)\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\ninputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}\r\nindices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0\r\n\r\n    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):\r\n        \"\"\"Utility to apply the function on a selection of columns.\"\"\"\r\n        nonlocal update_data\r\n        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]\r\n        if offset == 0:\r\n            effective_indices = indices\r\n        else:\r\n            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset\r\n        processed_inputs = (\r\n>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n        )\r\nE       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'\r\n\r\n../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError\r\n```\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.12.1\r\n- Platform: Mac\r\n- Python version: 3.8.9\r\n- PyArrow version: pyarrow==5.0.0\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/2927",
         "Fixed by #2950."
        ],
        [
         "18",
         "\"File name too long\" error for file locks",
         "## Describe the bug\r\n\r\nGetting the following error when calling `load_dataset(\"gar1t/test\")`:\r\n\r\n```\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nWhere the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"gar1t/test\")\r\n```\r\n\r\n## Expected results\r\n\r\nExpect the function to return without an error.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 765, in _save_info\r\n    with FileLock(lock_path):\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 323, in __enter__\r\n    self.acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 272, in acquire\r\n    self._acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 403, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2924",
         "Hi, the filename here is less than 255\r\n```python\r\n>>> len(\"_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock\")\r\n154\r\n```\r\nso not sure why it's considered too long for your filesystem.\r\n(also note that the lock files we use always have smaller filenames than 255)\r\n\r\nhttps://github.com/huggingface/datasets/blob/5d1a9f1e3c6c495dc0610b459e39d2eb8893f152/src/datasets/utils/filelock.py#L135-L135"
        ],
        [
         "19",
         "\"File name too long\" error for file locks",
         "## Describe the bug\r\n\r\nGetting the following error when calling `load_dataset(\"gar1t/test\")`:\r\n\r\n```\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Steps to reproduce the bug\r\n\r\nWhere the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"gar1t/test\")\r\n```\r\n\r\n## Expected results\r\n\r\nExpect the function to return without an error.\r\n\r\n## Actual results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/load.py\", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 644, in download_and_prepare\r\n    self._save_info()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/builder.py\", line 765, in _save_info\r\n    with FileLock(lock_path):\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 323, in __enter__\r\n    self.acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 272, in acquire\r\n    self._acquire()\r\n  File \"<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py\", line 403, in _acquire\r\n    fd = os.open(self._lock_file, open_mode)\r\nOSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.1\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.7\r\n- PyArrow version: 5.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2924",
         "Yes, you're right! I need to get you more info here. Either there's something going with the name itself that the file system doesn't like (an encoding that blows up the name length??) or perhaps there's something with the path that's causing the entire string to  be used as a name. I haven't seen this on any system before and the Internet's not forthcoming with any info."
        ],
        [
         "20",
         "Unwanted progress bars when accessing examples",
         "When accessing examples from a dataset formatted for pytorch, some progress bars appear when accessing examples:\r\n```python\r\nIn [1]: import datasets as ds                                        \r\n\r\nIn [2]: d = ds.Dataset.from_dict({\"a\": [0, 1, 2]}).with_format(\"torch\")                                                           \r\n\r\nIn [3]: d[0]                                                         \r\n100%|████████████████████████████████| 1/1 [00:00<00:00, 3172.70it/s]\r\nOut[3]: {'a': tensor(0)}\r\n```\r\n\r\nThis is because the pytorch formatter calls `map_nested` that uses progress bars\r\n\r\ncc @sgugger ",
         "https://github.com/huggingface/datasets/issues/2919",
         "doing a patch release now :)"
        ],
        [
         "21",
         "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
         "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in _iter(self)\r\n    336         else:\r\n    337             ex_iterable = self._ex_iterable\r\n--> 338         yield from ex_iterable\r\n    339\r\n    340     def __iter__(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n     76\r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80\r\n\r\n~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\scitldr\\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\\scitldr.py in _generate_examples(self, filepath, split)\r\n    162\r\n    163         with open(filepath, encoding=\"utf-8\") as f:\r\n--> 164             for id_, row in enumerate(f):\r\n    165                 data = json.loads(row)\r\n    166                 if self.config.name == \"AIC\":\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in read(self, length)\r\n    496         else:\r\n    497             length = min(self.size - self.loc, length)\r\n--> 498         return super().read(length)\r\n    499\r\n    500     async def async_fetch_all(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\spec.py in read(self, length)\r\n   1481             # don't even bother calling fetch\r\n   1482             return b\"\"\r\n-> 1483         out = self.cache._fetch(self.loc, self.loc + length)\r\n   1484         self.loc += len(out)\r\n   1485         return out\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\caching.py in _fetch(self, start, end)\r\n    378         elif start < self.start:\r\n    379             if self.end - end > self.blocksize:\r\n--> 380                 self.cache = self.fetcher(start, bend)\r\n    381                 self.start = start\r\n    382             else:\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in wrapper(*args, **kwargs)\r\n     86     def wrapper(*args, **kwargs):\r\n     87         self = obj or args[0]\r\n---> 88         return sync(self.loop, func, *args, **kwargs)\r\n     89\r\n     90     return wrapper\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     67         raise FSTimeoutError\r\n     68     if isinstance(result[0], BaseException):\r\n---> 69         raise result[0]\r\n     70     return result[0]\r\n     71\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in async_fetch_range(self, start, end)\r\n    538             if r.status == 206:\r\n    539                 # partial content, as expected\r\n--> 540                 out = await r.read()\r\n    541             elif \"Content-Length\" in r.headers:\r\n    542                 cl = int(r.headers[\"Content-Length\"])\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\client_reqrep.py in read(self)\r\n   1030         if self._body is None:\r\n   1031             try:\r\n-> 1032                 self._body = await self.content.read()\r\n   1033                 for trace in self._traces:\r\n   1034                     await trace.send_response_chunk_received(\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\streams.py in read(self, n)\r\n    342     async def read(self, n: int = -1) -> bytes:\r\n    343         if self._exception is not None:\r\n--> 344             raise self._exception\r\n    345\r\n    346         # migration problem; with DataQueue you have to catch\r\n\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n- aiohttp version: 3.7.4.post0\r\n",
         "https://github.com/huggingface/datasets/issues/2918",
         "Hi @SBrandeis, thanks for reporting! ^^\r\n\r\nI think this is an issue with `fsspec`: https://github.com/intake/filesystem_spec/issues/389\r\n\r\nI will ask them if they are planning to fix it..."
        ],
        [
         "22",
         "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
         "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in _iter(self)\r\n    336         else:\r\n    337             ex_iterable = self._ex_iterable\r\n--> 338         yield from ex_iterable\r\n    339\r\n    340     def __iter__(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n     76\r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80\r\n\r\n~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\scitldr\\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\\scitldr.py in _generate_examples(self, filepath, split)\r\n    162\r\n    163         with open(filepath, encoding=\"utf-8\") as f:\r\n--> 164             for id_, row in enumerate(f):\r\n    165                 data = json.loads(row)\r\n    166                 if self.config.name == \"AIC\":\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in read(self, length)\r\n    496         else:\r\n    497             length = min(self.size - self.loc, length)\r\n--> 498         return super().read(length)\r\n    499\r\n    500     async def async_fetch_all(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\spec.py in read(self, length)\r\n   1481             # don't even bother calling fetch\r\n   1482             return b\"\"\r\n-> 1483         out = self.cache._fetch(self.loc, self.loc + length)\r\n   1484         self.loc += len(out)\r\n   1485         return out\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\caching.py in _fetch(self, start, end)\r\n    378         elif start < self.start:\r\n    379             if self.end - end > self.blocksize:\r\n--> 380                 self.cache = self.fetcher(start, bend)\r\n    381                 self.start = start\r\n    382             else:\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in wrapper(*args, **kwargs)\r\n     86     def wrapper(*args, **kwargs):\r\n     87         self = obj or args[0]\r\n---> 88         return sync(self.loop, func, *args, **kwargs)\r\n     89\r\n     90     return wrapper\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     67         raise FSTimeoutError\r\n     68     if isinstance(result[0], BaseException):\r\n---> 69         raise result[0]\r\n     70     return result[0]\r\n     71\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in async_fetch_range(self, start, end)\r\n    538             if r.status == 206:\r\n    539                 # partial content, as expected\r\n--> 540                 out = await r.read()\r\n    541             elif \"Content-Length\" in r.headers:\r\n    542                 cl = int(r.headers[\"Content-Length\"])\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\client_reqrep.py in read(self)\r\n   1030         if self._body is None:\r\n   1031             try:\r\n-> 1032                 self._body = await self.content.read()\r\n   1033                 for trace in self._traces:\r\n   1034                     await trace.send_response_chunk_received(\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\streams.py in read(self, n)\r\n    342     async def read(self, n: int = -1) -> bytes:\r\n    343         if self._exception is not None:\r\n--> 344             raise self._exception\r\n    345\r\n    346         # migration problem; with DataQueue you have to catch\r\n\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n- aiohttp version: 3.7.4.post0\r\n",
         "https://github.com/huggingface/datasets/issues/2918",
         "Code to reproduce the bug: `ClientPayloadError: 400, message='Can not decode content-encoding: gzip'`\r\n```python\r\nIn [1]: import fsspec\r\n\r\nIn [2]: import json\r\n\r\nIn [3]: with fsspec.open('https://raw.githubusercontent.com/allenai/scitldr/master/SciTLDR-Data/SciTLDR-FullText/test.jsonl', encoding=\"utf-8\") as f:\r\n   ...:     for row in f:\r\n   ...:         data = json.loads(row)\r\n   ...:\r\n---------------------------------------------------------------------------\r\nClientPayloadError                        Traceback (most recent call last)\r\n```"
        ],
        [
         "23",
         "`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming",
         "## Describe the bug\r\n\r\nTrying to load the `\"FullText\"` config of the `\"scitldr\"` dataset with `streaming=True` raises an error from `aiohttp`:\r\n```python\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\ncc @lhoestq \r\n\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset\r\n\r\niter_dset = iter(\r\n    load_dataset(\"scitldr\", name=\"FullText\", split=\"test\", streaming=True)\r\n)\r\n\r\nnext(iter_dset)\r\n```\r\n\r\n## Expected results\r\nReturns the first sample of the dataset\r\n\r\n## Actual results\r\nCalling `__next__` crashes with the following Traceback:\r\n\r\n```python\r\n----> 1 next(dset_iter)\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n    339\r\n    340     def __iter__(self):\r\n--> 341         for key, example in self._iter():\r\n    342             if self.features:\r\n    343                 # we encode the example for ClassLabel feature types for example\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in _iter(self)\r\n    336         else:\r\n    337             ex_iterable = self._ex_iterable\r\n--> 338         yield from ex_iterable\r\n    339\r\n    340     def __iter__(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\datasets\\iterable_dataset.py in __iter__(self)\r\n     76\r\n     77     def __iter__(self):\r\n---> 78         for key, example in self.generate_examples_fn(**self.kwargs):\r\n     79             yield key, example\r\n     80\r\n\r\n~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\scitldr\\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\\scitldr.py in _generate_examples(self, filepath, split)\r\n    162\r\n    163         with open(filepath, encoding=\"utf-8\") as f:\r\n--> 164             for id_, row in enumerate(f):\r\n    165                 data = json.loads(row)\r\n    166                 if self.config.name == \"AIC\":\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in read(self, length)\r\n    496         else:\r\n    497             length = min(self.size - self.loc, length)\r\n--> 498         return super().read(length)\r\n    499\r\n    500     async def async_fetch_all(self):\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\spec.py in read(self, length)\r\n   1481             # don't even bother calling fetch\r\n   1482             return b\"\"\r\n-> 1483         out = self.cache._fetch(self.loc, self.loc + length)\r\n   1484         self.loc += len(out)\r\n   1485         return out\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\caching.py in _fetch(self, start, end)\r\n    378         elif start < self.start:\r\n    379             if self.end - end > self.blocksize:\r\n--> 380                 self.cache = self.fetcher(start, bend)\r\n    381                 self.start = start\r\n    382             else:\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in wrapper(*args, **kwargs)\r\n     86     def wrapper(*args, **kwargs):\r\n     87         self = obj or args[0]\r\n---> 88         return sync(self.loop, func, *args, **kwargs)\r\n     89\r\n     90     return wrapper\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in sync(loop, func, timeout, *args, **kwargs)\r\n     67         raise FSTimeoutError\r\n     68     if isinstance(result[0], BaseException):\r\n---> 69         raise result[0]\r\n     70     return result[0]\r\n     71\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\asyn.py in _runner(event, coro, result, timeout)\r\n     23         coro = asyncio.wait_for(coro, timeout=timeout)\r\n     24     try:\r\n---> 25         result[0] = await coro\r\n     26     except Exception as ex:\r\n     27         result[0] = ex\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\fsspec\\implementations\\http.py in async_fetch_range(self, start, end)\r\n    538             if r.status == 206:\r\n    539                 # partial content, as expected\r\n--> 540                 out = await r.read()\r\n    541             elif \"Content-Length\" in r.headers:\r\n    542                 cl = int(r.headers[\"Content-Length\"])\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\client_reqrep.py in read(self)\r\n   1030         if self._body is None:\r\n   1031             try:\r\n-> 1032                 self._body = await self.content.read()\r\n   1033                 for trace in self._traces:\r\n   1034                     await trace.send_response_chunk_received(\r\n\r\n~\\miniconda3\\envs\\datasets\\lib\\site-packages\\aiohttp\\streams.py in read(self, n)\r\n    342     async def read(self, n: int = -1) -> bytes:\r\n    343         if self._exception is not None:\r\n--> 344             raise self._exception\r\n    345\r\n    346         # migration problem; with DataQueue you have to catch\r\n\r\nClientPayloadError: 400, message='Can not decode content-encoding: gzip'\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.8.5\r\n- PyArrow version: 2.0.0\r\n- aiohttp version: 3.7.4.post0\r\n",
         "https://github.com/huggingface/datasets/issues/2918",
         "Thanks for investigating @albertvillanova ! 🤗 "
        ],
        [
         "24",
         "windows download abnormal",
         "## Describe the bug\r\nThe script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??\r\n## Steps to reproduce the bug\r\n```python3.7 + windows\r\n![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)\r\n\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nIt can be downloaded normally.\r\n\r\n## Actual results\r\nit cann't\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.11.0\r\n- Platform:windows\r\n- Python version:3.7\r\n- PyArrow version:\r\n",
         "https://github.com/huggingface/datasets/issues/2917",
         "Hi ! Is there some kind of proxy that is configured in your browser that gives you access to internet ? If it's the case it could explain why it doesn't work in the code, since the proxy wouldn't be used"
        ],
        [
         "25",
         "windows download abnormal",
         "## Describe the bug\r\nThe script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??\r\n## Steps to reproduce the bug\r\n```python3.7 + windows\r\n![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)\r\n\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nIt can be downloaded normally.\r\n\r\n## Actual results\r\nit cann't\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.11.0\r\n- Platform:windows\r\n- Python version:3.7\r\n- PyArrow version:\r\n",
         "https://github.com/huggingface/datasets/issues/2917",
         "It is indeed an agency problem, thank you very, very much"
        ],
        [
         "26",
         "windows download abnormal",
         "## Describe the bug\r\nThe script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??\r\n## Steps to reproduce the bug\r\n```python3.7 + windows\r\n![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)\r\n\r\n\r\n# Sample code to reproduce the bug\r\n```\r\n\r\n## Expected results\r\nIt can be downloaded normally.\r\n\r\n## Actual results\r\nit cann't\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version:1.11.0\r\n- Platform:windows\r\n- Python version:3.7\r\n- PyArrow version:\r\n",
         "https://github.com/huggingface/datasets/issues/2917",
         "Let me know if you have other questions :)\r\n\r\nClosing this issue now"
        ],
        [
         "27",
         "Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets",
         "## Describe the bug\r\nIn one of my project, I defined a custom fsspec filesystem with an entrypoint.\r\nMy guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).\r\nSo that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.\r\nThis make the import of datasets failing as it is using that `fsspec.spec`.\r\n\r\n## Steps to reproduce the bug\r\nI could reproduce the bug with a dummy poetry project.\r\n\r\nHere is the pyproject.toml:\r\n```toml\r\n[tool.poetry]\r\nname = \"debug-datasets\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = [\"Pierre Godard\"]\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.8\"\r\ndatasets = \"^1.11.0\"\r\n\r\n[tool.poetry.dev-dependencies]\r\n\r\n[build-system]\r\nrequires = [\"poetry-core>=1.0.0\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n\r\n[tool.poetry.plugins.\"fsspec.specs\"]\r\n\"file2\" = \"fsspec.implementations.local.LocalFileSystem\"\r\n```\r\n\r\nThe only other file being a `debug_datasets/__init__.py` empty file.\r\n\r\nThe overall structure of the project is as follows:\r\n```\r\n.\r\n├── pyproject.toml\r\n└── debug_datasets\r\n    └── __init__.py\r\n```\r\n\r\nThen, within the project folder run:\r\n\r\n```\r\npoetry install\r\npoetry run python\r\n```\r\n\r\nAnd in the python interpreter, try to import `datasets`:\r\n\r\n```\r\nimport datasets\r\n```\r\n\r\n## Expected results\r\nThe import should run successfully.\r\n\r\n## Actual results\r\n\r\nHere is the trace of the error I get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/__init__.py\", line 33, in <module>\r\n    from .arrow_dataset import Dataset, concatenate_datasets\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 48, in <module>\r\n    from .filesystems import extract_path_from_uri, is_remote_filesystem\r\n  File \"/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/filesystems/__init__.py\", line 30, in <module>\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\nAttributeError: 'EntryPoint' object has no attribute 'AbstractFileSystem'\r\n```\r\n\r\n## Suggested fix\r\n\r\n`datasets/filesystems/__init__.py`, line 30, replace:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:\r\n```\r\nby:\r\n```\r\n    def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:\r\n```\r\n\r\nI will come up with a PR soon if this effectively solves the issue.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform: WSL2 (Ubuntu 20.04.1 LTS)\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n- `fsspec` version: 2021.8.1\r\n",
         "https://github.com/huggingface/datasets/issues/2914",
         "Closed by #2915."
        ],
        [
         "28",
         "timit_asr dataset only includes one text phrase",
         "## Describe the bug\r\nThe dataset 'timit_asr' only includes one text phrase. It only includes the transcription \"Would such an act of refusal be useful?\" multiple times rather than different phrases.\r\n\r\n## Steps to reproduce the bug\r\nNote: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\n1. Install the dataset and other packages\r\n```python\r\n!pip install datasets>=1.5.0\r\n!pip install transformers==4.4.0\r\n!pip install soundfile\r\n!pip install jiwer\r\n```\r\n2. Load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\n```\r\n3. Remove columns that we don't want\r\n```python\r\ntimit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])\r\n```\r\n4. Write a short function to display some random samples of the dataset.\r\n```python\r\nfrom datasets import ClassLabel\r\nimport random\r\nimport pandas as pd\r\nfrom IPython.display import display, HTML\r\n\r\ndef show_random_elements(dataset, num_examples=10):\r\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n    picks = []\r\n    for _ in range(num_examples):\r\n        pick = random.randint(0, len(dataset)-1)\r\n        while pick in picks:\r\n            pick = random.randint(0, len(dataset)-1)\r\n        picks.append(pick)\r\n    \r\n    df = pd.DataFrame(dataset[picks])\r\n    display(HTML(df.to_html()))\r\n\r\nshow_random_elements(timit[\"train\"].remove_columns([\"file\"]))\r\n```\r\n\r\n## Expected results\r\n10 random different transcription phrases.\r\n\r\n## Actual results\r\n10 of the same transcription phrase \"Would such an act of refusal be useful?\"\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.4.1\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: not listed\r\n",
         "https://github.com/huggingface/datasets/issues/2913",
         "Hi @margotwagner, \r\nThis bug was fixed in #1995. Upgrading the datasets should work (min v1.8.0 ideally)"
        ],
        [
         "29",
         "timit_asr dataset only includes one text phrase",
         "## Describe the bug\r\nThe dataset 'timit_asr' only includes one text phrase. It only includes the transcription \"Would such an act of refusal be useful?\" multiple times rather than different phrases.\r\n\r\n## Steps to reproduce the bug\r\nNote: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\n1. Install the dataset and other packages\r\n```python\r\n!pip install datasets>=1.5.0\r\n!pip install transformers==4.4.0\r\n!pip install soundfile\r\n!pip install jiwer\r\n```\r\n2. Load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\n\r\ntimit = load_dataset(\"timit_asr\")\r\n```\r\n3. Remove columns that we don't want\r\n```python\r\ntimit = timit.remove_columns([\"phonetic_detail\", \"word_detail\", \"dialect_region\", \"id\", \"sentence_type\", \"speaker_id\"])\r\n```\r\n4. Write a short function to display some random samples of the dataset.\r\n```python\r\nfrom datasets import ClassLabel\r\nimport random\r\nimport pandas as pd\r\nfrom IPython.display import display, HTML\r\n\r\ndef show_random_elements(dataset, num_examples=10):\r\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\r\n    picks = []\r\n    for _ in range(num_examples):\r\n        pick = random.randint(0, len(dataset)-1)\r\n        while pick in picks:\r\n            pick = random.randint(0, len(dataset)-1)\r\n        picks.append(pick)\r\n    \r\n    df = pd.DataFrame(dataset[picks])\r\n    display(HTML(df.to_html()))\r\n\r\nshow_random_elements(timit[\"train\"].remove_columns([\"file\"]))\r\n```\r\n\r\n## Expected results\r\n10 random different transcription phrases.\r\n\r\n## Actual results\r\n10 of the same transcription phrase \"Would such an act of refusal be useful?\"\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.4.1\r\n- Platform: macOS-10.15.7-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- PyArrow version: not listed\r\n",
         "https://github.com/huggingface/datasets/issues/2913",
         "Hi @margotwagner,\r\n\r\nYes, as @bhavitvyamalik has commented, this bug was fixed in `datasets` version 1.5.0. You need to update it, as your current version is 1.4.1:\r\n> Environment info\r\n> - `datasets` version: 1.4.1"
        ],
        [
         "30",
         "FORCE_REDOWNLOAD does not work",
         "## Describe the bug\r\nWith GenerateMode.FORCE_REDOWNLOAD, the documentation says \r\n    +------------------------------------+-----------+---------+\r\n    |                                    | Downloads | Dataset |\r\n    +====================================+===========+=========+\r\n    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\r\n    +------------------------------------+-----------+---------+\r\n    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\r\n    +------------------------------------+-----------+---------+\r\n    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\r\n    +------------------------------------+-----------+---------+\r\n\r\nHowever, the old dataset is loaded even when FORCE_REDOWNLOAD is chosen.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n\r\nimport pandas as pd\r\nfrom datasets import load_dataset, GenerateMode\r\npd.DataFrame(range(5), columns=['numbers']).to_csv('/tmp/test.tsv.gz', index=False)\r\nee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)\r\nprint(ee)\r\npd.DataFrame(range(10), columns=['numerals']).to_csv('/tmp/test.tsv.gz', index=False)\r\nee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)\r\nprint(ee)\r\n\r\n```\r\n\r\n## Expected results\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['numerals'],\r\n    num_rows: 10\r\n})\r\n\r\n## Actual results\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\nDataset({\r\n    features: ['numbers'],\r\n    num_rows: 5\r\n})\r\n\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.8.0\r\n- Platform: Linux-4.14.181-108.257.amzn1.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.7.10\r\n- PyArrow version: 3.0.0\r\n",
         "https://github.com/huggingface/datasets/issues/2904",
         "Hi ! Thanks for reporting. The error seems to happen only if you use compressed files.\r\n\r\nThe second dataset is prepared in another dataset cache directory than the first - which is normal, since the source file is different. However, it doesn't uncompress the new data file because it finds the old uncompressed data in the extraction cache directory.\r\n\r\nIf we fix the extraction cache mechanism to uncompress a local file if it changed then it should fix the issue.\r\nCurrently the extraction cache mechanism only takes into account the path of the compressed file, which is an issue."
        ],
        [
         "31",
         "Add WIT Dataset",
         "## Adding a Dataset\r\n- **Name:** *WIT*\r\n- **Description:** *Wikipedia-based Image Text Dataset*\r\n- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\r\n](https://arxiv.org/abs/2103.01913)*\r\n- **Data:** *https://github.com/google-research-datasets/wit*\r\n- **Motivation:**  (excerpt from their Github README.md)\r\n\r\n> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.\r\n> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.\r\n> - A collection of diverse set of concepts and real world entities.\r\n> - Brings forth challenging real-world test sets.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
         "https://github.com/huggingface/datasets/issues/2902",
         "@hassiahk is working on it  #2810 "
        ],
        [
         "32",
         "Add WIT Dataset",
         "## Adding a Dataset\r\n- **Name:** *WIT*\r\n- **Description:** *Wikipedia-based Image Text Dataset*\r\n- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\r\n](https://arxiv.org/abs/2103.01913)*\r\n- **Data:** *https://github.com/google-research-datasets/wit*\r\n- **Motivation:**  (excerpt from their Github README.md)\r\n\r\n> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.\r\n> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.\r\n> - A collection of diverse set of concepts and real world entities.\r\n> - Brings forth challenging real-world test sets.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
         "https://github.com/huggingface/datasets/issues/2902",
         "WikiMedia is now hosting the pixel values directly which should make it a lot easier!\r\nThe files can be found here:\r\nhttps://techblog.wikimedia.org/2021/09/09/the-wikipedia-image-caption-matching-challenge-and-a-huge-release-of-image-data-for-research/\r\nhttps://analytics.wikimedia.org/published/datasets/one-off/caption_competition/training/image_pixels/"
        ],
        [
         "33",
         "Add WIT Dataset",
         "## Adding a Dataset\r\n- **Name:** *WIT*\r\n- **Description:** *Wikipedia-based Image Text Dataset*\r\n- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\r\n](https://arxiv.org/abs/2103.01913)*\r\n- **Data:** *https://github.com/google-research-datasets/wit*\r\n- **Motivation:**  (excerpt from their Github README.md)\r\n\r\n> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.\r\n> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.\r\n> - A collection of diverse set of concepts and real world entities.\r\n> - Brings forth challenging real-world test sets.\r\n\r\nInstructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n",
         "https://github.com/huggingface/datasets/issues/2902",
         "> @hassiahk is working on it #2810\r\n\r\nThank you @bhavitvyamalik! Added this issue so we could track progress 😄 . Just linked the PR as well for visibility. "
        ],
        [
         "34",
         "Incompatibility with pytest",
         "## Describe the bug\r\n\r\npytest complains about xpathopen / path.open(\"w\")\r\n\r\n## Steps to reproduce the bug\r\n\r\nCreate a test file, `test.py`:\r\n\r\n```python\r\nimport datasets as ds\r\ndef load_dataset():\r\n    ds.load_dataset(\"counter\", split=\"train\", streaming=True)\r\n```\r\n\r\nAnd launch it with pytest:\r\n\r\n```bash\r\npython -m pytest test.py\r\n```\r\n\r\n## Expected results\r\n\r\nIt should give something like:\r\n\r\n```\r\ncollected 1 item\r\n\r\ntest.py .                                                                                                                                                                                                                                             [100%]\r\n\r\n======= 1 passed in 3.15s =======\r\n```\r\n\r\n## Actual results\r\n\r\n```\r\n============================================================================================================================= test session starts ==============================================================================================================================\r\nplatform linux -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\r\nrootdir: /home/slesage/hf/datasets-preview-backend, configfile: pyproject.toml\r\nplugins: anyio-3.3.1\r\ncollected 1 item\r\n\r\ntests/queries/test_rows.py .                                                                                                                                                                                                                                             [100%]Traceback (most recent call last):\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pytest/__main__.py\", line 5, in <module>\r\n    raise SystemExit(pytest.console_main())\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 185, in console_main\r\n    code = main()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py\", line 162, in main\r\n    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 60, in _multicall\r\n    return outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 316, in pytest_cmdline_main\r\n    return wrap_session(config, _main)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py\", line 304, in wrap_session\r\n    config.hook.pytest_sessionfinish(\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py\", line 265, in __call__\r\n    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py\", line 80, in _hookexec\r\n    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 55, in _multicall\r\n    gen.send(outcome)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/terminal.py\", line 803, in pytest_sessionfinish\r\n    outcome.get_result()\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py\", line 60, in get_result\r\n    raise ex[1].with_traceback(ex[2])\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py\", line 39, in _multicall\r\n    res = hook_impl.function(*args)\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 428, in pytest_sessionfinish\r\n    config.cache.set(\"cache/nodeids\", sorted(self.cached_nodeids))\r\n  File \"/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py\", line 188, in set\r\n    f = path.open(\"w\")\r\nTypeError: xpathopen() takes 1 positional argument but 2 were given\r\n```\r\n\r\n## Environment info\r\n\r\n- `datasets` version: 1.12.0\r\n- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.11\r\n- PyArrow version: 4.0.1\r\n",
         "https://github.com/huggingface/datasets/issues/2901",
         "Sorry, my bad... When implementing `xpathopen`, I just considered the use case in the COUNTER dataset... I'm fixing it!"
        ],
        [
         "35",
         "Error when encoding a dataset with None objects with a Sequence feature",
         "There is an error when encoding a dataset with None objects with a Sequence feature\r\n\r\nTo reproduce:\r\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence\r\ndata = {\"a\": [[0], None]}\r\nfeatures = Features({\"a\": Sequence(Value(\"int32\"))})\r\ndataset = Dataset.from_dict(data, features=features)\r\n```\r\nraises\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-24-40add67f8751> in <module>\r\n      2 data = {\"a\": [[0], None]}\r\n      3 features = Features({\"a\": Sequence(Value(\"int32\"))})\r\n----> 4 dataset = Dataset.from_dict(data, features=features)\r\n[...]\r\n~/datasets/features.py in encode_nested_example(schema, obj)\r\n    888         if isinstance(obj, str):  # don't interpret a string as a list\r\n    889             raise ValueError(\"Got a string but expected a list instead: '{}'\".format(obj))\r\n--> 890         return [encode_nested_example(schema.feature, o) for o in obj]\r\n    891     # Object with special encoding:\r\n    892     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\r\n\r\nTypeError: 'NoneType' object is not iterable\r\n```\r\n\r\nInstead, if should run without error, as if the `features` were not passed",
         "https://github.com/huggingface/datasets/issues/2892",
         "This has been fixed by https://github.com/huggingface/datasets/pull/2900\r\nWe're doing a new release 1.12 today to make the fix available :)"
        ],
        [
         "36",
         "v1.11.1 release date",
         "Hello, i need to use latest features in one of my packages but there have been no new datasets release since 2 months ago.\r\n\r\nWhen do you plan to publush v1.11.1 release?",
         "https://github.com/huggingface/datasets/issues/2888",
         "Hi ! Probably 1.12 on monday :)\r\n"
        ],
        [
         "37",
         "v1.11.1 release date",
         "Hello, i need to use latest features in one of my packages but there have been no new datasets release since 2 months ago.\r\n\r\nWhen do you plan to publush v1.11.1 release?",
         "https://github.com/huggingface/datasets/issues/2888",
         "@albertvillanova i think this issue is still valid and should not be closed till `>1.11.0` is published :)"
        ],
        [
         "38",
         "Adding an Elastic Search index to a Dataset",
         "## Describe the bug\r\nWhen trying to index documents from the squad dataset, the connection to ElasticSearch seems to break:\r\n\r\nReusing dataset squad (/Users/andreasmotz/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\r\n 90%|████████████████████████████████████████████▉     | 9501/10570 [00:01<00:00, 6335.61docs/s]\r\n\r\nNo error is thrown, but the indexing breaks ~90%.\r\n\r\n## Steps to reproduce the bug\r\n```python\r\n# Sample code to reproduce the bug\r\nfrom datasets import load_dataset\r\nfrom elasticsearch import Elasticsearch\r\nes = Elasticsearch()\r\nsquad = load_dataset('squad', split='validation')\r\nindex_name = \"corpus\"\r\nes_config = {\r\n    \"settings\": {\r\n        \"number_of_shards\": 1,\r\n        \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\r\n    },\r\n    \"mappings\": {\r\n        \"properties\": {\r\n            \"idx\" : {\"type\" : \"keyword\"},\r\n            \"title\" : {\"type\" : \"keyword\"},\r\n            \"text\": {\r\n                \"type\": \"text\",\r\n                \"analyzer\": \"standard\",\r\n                \"similarity\": \"BM25\"\r\n            },\r\n        }\r\n    },\r\n}\r\nclass IndexBuilder:\r\n    \"\"\"\r\n    Elastic search indexing of a corpus\r\n    \"\"\"\r\n    def __init__(\r\n        self,\r\n        *args,\r\n        #corpus : None,\r\n        dataset : squad,\r\n        index_name = str,\r\n        query = str,\r\n        config = dict,\r\n        **kwargs,\r\n    ):\r\n        #instantiate HuggingFace dataset\r\n        self.dataset = dataset\r\n        #instantiate ElasticSearch config\r\n        self.config = config\r\n        self.es = Elasticsearch()\r\n        self.index_name = index_name\r\n        self.query = query\r\n    def elastic_index(self):\r\n        print(self.es.info)\r\n        self.es.indices.delete(index=self.index_name, ignore=[400, 404])\r\n        search_index = self.dataset.add_elasticsearch_index(column='context', host='localhost', port='9200', es_index_name=self.index_name, es_index_config=self.config)\r\n        return search_index\r\n    def exact_match_method(self, index):\r\n        scores, retrieved_examples = index.get_nearest_examples('context', query=self.query, k=1)\r\n        return scores, retrieved_examples\r\nif __name__ == \"__main__\":\r\n    print(type(squad))\r\n    Index = IndexBuilder(dataset=squad, index_name='corpus_index', query='Where was Chopin born?', config=es_config)\r\n    search_index = Index.elastic_index()\r\n    scores, examples = Index.exact_match_method(search_index)\r\n    print(scores, examples)\r\n    for name in squad.column_names:\r\n        print(type(squad[name]))\r\n```\r\n\r\n## Environment info\r\nWe run the code in Poetry. This might be the issue, since the script runs successfully in our local environment.\r\n\r\nPoetry:\r\n- Python version: 3.8\r\n- PyArrow: 4.0.1\r\n- Elasticsearch: 7.13.4\r\n- datasets: 1.10.2\r\n\r\nLocal:\r\n- Python version: 3.8\r\n- PyArrow: 3.0.0\r\n- Elasticsearch: 7.7.1\r\n- datasets: 1.7.0\r\n",
         "https://github.com/huggingface/datasets/issues/2885",
         "Hi, is this bug deterministic in your poetry env ? I mean, does it always stop at 90% or is it random ?\r\n\r\nAlso, can you try using another version of Elasticsearch ? Maybe there's an issue with the one of you poetry env"
        ],
        [
         "39",
         "`load_dataset('docred')` results in a `NonMatchingChecksumError` ",
         "## Describe the bug\r\nI get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.\r\n\r\n## Steps to reproduce the bug\r\nIt is quasi only this code:\r\n```python\r\nimport datasets\r\ndata = datasets.load_dataset('docred')\r\n```\r\n\r\n## Expected results\r\nThe DocRED dataset should be loaded without any problems.\r\n\r\n## Actual results\r\n```\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-4-b1b83f25a16c> in <module>\r\n----> 1 d = datasets.load_dataset('docred')\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    845 \r\n    846     # Download and prepare data\r\n--> 847     builder_instance.download_and_prepare(\r\n    848         download_config=download_config,\r\n    849         download_mode=download_mode,\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    613                             logger.warning(\"HF google storage unreachable. Downloading and preparing it from source\")\r\n    614                     if not downloaded_from_gcs:\r\n--> 615                         self._download_and_prepare(\r\n    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    617                         )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    673         # Checksums verification\r\n    674         if verify_infos:\r\n--> 675             verify_checksums(\r\n    676                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), \"dataset source files\"\r\n    677             )\r\n\r\n~/anaconda3/lib/python3.8/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = \"Checksums didn't match\" + for_verification_name + \":\\n\"\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(\"All the checksums matched successfully\" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://drive.google.com/uc?export=download&id=1fDmfUUo5G7gfaoqWWvK81u08m71TK2g7']\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Linux-5.11.0-7633-generic-x86_64-with-glibc2.10\r\n- Python version: 3.8.5\r\n- PyArrow version: 5.0.0\r\n\r\nThis error also happened on my Windows-partition, after freshly installing python 3.9 and `datasets`.\r\n\r\n## Remarks\r\n\r\n- I have already called `rm -rf /home/<user>/.cache/huggingface`, i.e., I have tried clearing the cache.\r\n- The problem does not exist for other datasets, i.e., it seems to be DocRED-specific.",
         "https://github.com/huggingface/datasets/issues/2882",
         "Hi @tmpr, thanks for reporting.\r\n\r\nTwo weeks ago (23th Aug), the host of the source `docred` dataset updated one of the files (`dev.json`): you can see it [here](https://drive.google.com/drive/folders/1c5-0YwnoJx8NS6CV2f-NoTHR__BdkNqw).\r\n\r\nTherefore, the checksum needs to be updated.\r\n\r\nNormally, in the meantime, you could avoid the error by passing `ignore_verifications=True` to `load_dataset`. However, as the old link points to a non-existing file, the link must be updated too.\r\n\r\nI'm fixing all this.\r\n\r\n"
        ],
        [
         "40",
         "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"",
         "## Describe the bug\r\nUsing version 1.4.1 of `datasets`, TIMIT transcripts are all the same.\r\n\r\n## Steps to reproduce the bug\r\nI was following this tutorial\r\n- https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\nBut here's a distilled repro:\r\n```python\r\n!pip install datasets==1.4.1\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\", cache_dir=\"./temp\")\r\nunique_transcripts = set(timit[\"train\"][\"text\"])\r\nprint(unique_transcripts)\r\nassert len(unique_transcripts) > 1\r\n```\r\n## Expected results\r\nExpected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.\r\n\r\n## Actual results\r\nEvery train transcript was \"Would such an act of refusal be useful?\" Every test transcript was \"The bungalow was pleasantly situated near the shore.\"\r\n\r\n## Environment info\r\n- `datasets` version: 1.4.1\r\n- Platform: Darwin-18.7.0-x86_64-i386-64bit\r\n- Python version: 3.7.9\r\n- PyTorch version (GPU?): 1.9.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: tried both\r\n- Using distributed or parallel set-up in script?: no\r\n- \r\n\r\n",
         "https://github.com/huggingface/datasets/issues/2879",
         "Hi @rcgale, thanks for reporting.\r\n\r\nPlease note that this bug was fixed on `datasets` version 1.5.0: https://github.com/huggingface/datasets/commit/a23c73e526e1c30263834164f16f1fdf76722c8c#diff-f12a7a42d4673bb6c2ca5a40c92c29eb4fe3475908c84fd4ce4fad5dc2514878\r\n\r\nIf you update `datasets` version, that should work.\r\n\r\nOn the other hand, would it be possible for @patrickvonplaten to update the [blog post](https://huggingface.co/blog/fine-tune-wav2vec2-english) with the correct version of `datasets`?"
        ],
        [
         "41",
         "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"",
         "## Describe the bug\r\nUsing version 1.4.1 of `datasets`, TIMIT transcripts are all the same.\r\n\r\n## Steps to reproduce the bug\r\nI was following this tutorial\r\n- https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\nBut here's a distilled repro:\r\n```python\r\n!pip install datasets==1.4.1\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\", cache_dir=\"./temp\")\r\nunique_transcripts = set(timit[\"train\"][\"text\"])\r\nprint(unique_transcripts)\r\nassert len(unique_transcripts) > 1\r\n```\r\n## Expected results\r\nExpected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.\r\n\r\n## Actual results\r\nEvery train transcript was \"Would such an act of refusal be useful?\" Every test transcript was \"The bungalow was pleasantly situated near the shore.\"\r\n\r\n## Environment info\r\n- `datasets` version: 1.4.1\r\n- Platform: Darwin-18.7.0-x86_64-i386-64bit\r\n- Python version: 3.7.9\r\n- PyTorch version (GPU?): 1.9.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: tried both\r\n- Using distributed or parallel set-up in script?: no\r\n- \r\n\r\n",
         "https://github.com/huggingface/datasets/issues/2879",
         "I just proposed a change in the blog post.\r\n\r\nI had assumed there was a data format change that broke a previous version of the code, since presumably @patrickvonplaten tested the tutorial with the version they explicitly referenced. But that fix you linked suggests a problem in the code, which surprised me.\r\n\r\nI still wonder, though, is there a way for downloads to be invalidated server-side? If the client can announce its version during a download request, perhaps the server could reject known incompatibilities? It would save much valuable time if `datasets` raised an informative error on a known problem (\"Error: the requested data set requires `datasets>=1.5.0`.\"). This kind of API versioning is a prudent move anyhow, as there will surely come a time when you'll need to make a breaking change to data."
        ],
        [
         "42",
         "In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"",
         "## Describe the bug\r\nUsing version 1.4.1 of `datasets`, TIMIT transcripts are all the same.\r\n\r\n## Steps to reproduce the bug\r\nI was following this tutorial\r\n- https://huggingface.co/blog/fine-tune-wav2vec2-english\r\n\r\nBut here's a distilled repro:\r\n```python\r\n!pip install datasets==1.4.1\r\nfrom datasets import load_dataset\r\ntimit = load_dataset(\"timit_asr\", cache_dir=\"./temp\")\r\nunique_transcripts = set(timit[\"train\"][\"text\"])\r\nprint(unique_transcripts)\r\nassert len(unique_transcripts) > 1\r\n```\r\n## Expected results\r\nExpected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.\r\n\r\n## Actual results\r\nEvery train transcript was \"Would such an act of refusal be useful?\" Every test transcript was \"The bungalow was pleasantly situated near the shore.\"\r\n\r\n## Environment info\r\n- `datasets` version: 1.4.1\r\n- Platform: Darwin-18.7.0-x86_64-i386-64bit\r\n- Python version: 3.7.9\r\n- PyTorch version (GPU?): 1.9.0 (False)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Using GPU in script?: tried both\r\n- Using distributed or parallel set-up in script?: no\r\n- \r\n\r\n",
         "https://github.com/huggingface/datasets/issues/2879",
         "Also, thank you for a quick and helpful reply!"
        ],
        [
         "43",
         "datasets.config.PYARROW_VERSION has no attribute 'major'",
         "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
         "https://github.com/huggingface/datasets/issues/2871",
         "I have changed line 288 to `if int(datasets.config.PYARROW_VERSION.split(\".\")[0]) < 3:` just to get around it."
        ],
        [
         "44",
         "datasets.config.PYARROW_VERSION has no attribute 'major'",
         "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
         "https://github.com/huggingface/datasets/issues/2871",
         "Hi @bwang482,\r\n\r\nI'm sorry but I'm not able to reproduce your bug.\r\n\r\nPlease note that in our current master branch, we made a commit (d03223d4d64b89e76b48b00602aba5aa2f817f1e) that simultaneously modified:\r\n- test_dataset_common.py: https://github.com/huggingface/datasets/commit/d03223d4d64b89e76b48b00602aba5aa2f817f1e#diff-a1bc225bd9a5bade373d1f140e24d09cbbdc97971c2f73bb627daaa803ada002L289 that introduces the usage of `datasets.config.PYARROW_VERSION.major`\r\n- but also changed config.py: https://github.com/huggingface/datasets/commit/d03223d4d64b89e76b48b00602aba5aa2f817f1e#diff-e021fcfc41811fb970fab889b8d245e68382bca8208e63eaafc9a396a336f8f2L40, so that `datasets.config.PYARROW_VERSION.major` exists\r\n"
        ],
        [
         "45",
         "datasets.config.PYARROW_VERSION has no attribute 'major'",
         "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
         "https://github.com/huggingface/datasets/issues/2871",
         "Sorted. Thanks!"
        ],
        [
         "46",
         "datasets.config.PYARROW_VERSION has no attribute 'major'",
         "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
         "https://github.com/huggingface/datasets/issues/2871",
         "Reopening this. Although the `test_dataset_common.py` script works fine now.\r\n\r\nHas this got something to do with my pull request not passing `ci/circleci: run_dataset_script_tests_pyarrow` tests?\r\n\r\nhttps://github.com/huggingface/datasets/pull/2873"
        ],
        [
         "47",
         "datasets.config.PYARROW_VERSION has no attribute 'major'",
         "In the test_dataset_common.py script, line 288-289\r\n\r\n```\r\nif datasets.config.PYARROW_VERSION.major < 3:\r\n   packaged_datasets = [pd for pd in packaged_datasets if pd[\"dataset_name\"] != \"parquet\"]\r\n```\r\n\r\nwhich throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.\r\n\r\n```\r\nimport datasets\r\ndatasets.config.PYARROW_VERSION.major\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>\r\n      1 import datasets\r\n----> 2 datasets.config.PYARROW_VERSION.major\r\n\r\nAttributeError: 'str' object has no attribute 'major'\r\n```\r\n\r\n## Environment info\r\n- `datasets` version: 1.11.0\r\n- Platform: Darwin-20.6.0-x86_64-i386-64bit\r\n- Python version: 3.7.11\r\n- PyArrow version: 4.0.1\r\n",
         "https://github.com/huggingface/datasets/issues/2871",
         "Hi @bwang482,\r\n\r\nIf you click on `Details` (on the right of your non passing CI test names: `ci/circleci: run_dataset_script_tests_pyarrow`), you can have more information about the non-passing tests.\r\n\r\nFor example, for [\"ci/circleci: run_dataset_script_tests_pyarrow_1\" details](https://circleci.com/gh/huggingface/datasets/46324?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link), you can see that the only non-passing test has to do with the dataset card (missing information in the `README.md` file): `test_changed_dataset_card`\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_dataset_cards.py::test_changed_dataset_card[swedish_medical_ner]\r\n= 1 failed, 3214 passed, 2874 skipped, 2 xfailed, 1 xpassed, 15 warnings in 175.59s (0:02:55) =\r\n```\r\n\r\nTherefore, your PR non-passing test has nothing to do with this issue."
        ],
        [
         "48",
         "TypeError: 'NoneType' object is not callable",
         "## Describe the bug\r\n\r\nTypeError: 'NoneType' object is not callable\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\ndataset = datasets.load_dataset(\"glue\", 'cola')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform:\r\n- Python version: 3.7\r\n- PyArrow version:\r\n",
         "https://github.com/huggingface/datasets/issues/2869",
         "Hi, @Chenfei-Kang.\r\n\r\nI'm sorry, but I'm not able to reproduce your bug:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", 'cola')\r\nds\r\n```\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 8551\r\n    })\r\n    validation: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1043\r\n    })\r\n    test: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1063\r\n    })\r\n})\r\n```\r\n\r\nCould you please give more details and environment info (platform, PyArrow version)?"
        ],
        [
         "49",
         "TypeError: 'NoneType' object is not callable",
         "## Describe the bug\r\n\r\nTypeError: 'NoneType' object is not callable\r\n## Steps to reproduce the bug\r\n```python\r\nfrom datasets import load_dataset, load_metric\r\ndataset = datasets.load_dataset(\"glue\", 'cola')\r\n```\r\n\r\n## Expected results\r\nA clear and concise description of the expected results.\r\n\r\n## Actual results\r\nSpecify the actual results or traceback.\r\n\r\n## Environment info\r\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\r\n- `datasets` version: 1.11.0\r\n- Platform:\r\n- Python version: 3.7\r\n- PyArrow version:\r\n",
         "https://github.com/huggingface/datasets/issues/2869",
         "> Hi, @Chenfei-Kang.\r\n> \r\n> I'm sorry, but I'm not able to reproduce your bug:\r\n> \r\n> ```python\r\n> from datasets import load_dataset\r\n> \r\n> ds = load_dataset(\"glue\", 'cola')\r\n> ds\r\n> ```\r\n> \r\n> ```\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: ['sentence', 'label', 'idx'],\r\n>         num_rows: 8551\r\n>     })\r\n>     validation: Dataset({\r\n>         features: ['sentence', 'label', 'idx'],\r\n>         num_rows: 1043\r\n>     })\r\n>     test: Dataset({\r\n>         features: ['sentence', 'label', 'idx'],\r\n>         num_rows: 1063\r\n>     })\r\n> })\r\n> ```\r\n> \r\n> Could you please give more details and environment info (platform, PyArrow version)?\r\n\r\nSorry to reply you so late.\r\nplatform: pycharm 2021 + anaconda with python 3.7\r\nPyArrow version: 5.0.0\r\nhuggingface-hub: 0.0.16\r\ndatasets: 1.9.0\r\n"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2964
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Well it can cause issue with anyone that updat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>My first bug report ❤️\\r\\nLooking into this ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>Ok, there are some news, most good than bad :l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>Ok great, so as discussed today, let's:\\r\\n- h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>Good plan!\\r\\n\\r\\nYes I do use `builder_kwargs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>Done!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2964 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0                                 Protect master branch   \n",
       "1                                 Protect master branch   \n",
       "2     Backwards compatibility broken for cached data...   \n",
       "3     Backwards compatibility broken for cached data...   \n",
       "4     Backwards compatibility broken for cached data...   \n",
       "...                                                 ...   \n",
       "2959                      Issue to read a local dataset   \n",
       "2960                      Issue to read a local dataset   \n",
       "2961                      Issue to read a local dataset   \n",
       "2962                      Issue to read a local dataset   \n",
       "2963                      Issue to read a local dataset   \n",
       "\n",
       "                                                   body  \\\n",
       "0     After accidental merge commit (91c55355b634d0d...   \n",
       "1     After accidental merge commit (91c55355b634d0d...   \n",
       "2     ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "3     ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "4     ## Describe the bug\\r\\nAfter upgrading to data...   \n",
       "...                                                 ...   \n",
       "2959  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...   \n",
       "2960  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...   \n",
       "2961  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...   \n",
       "2962  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...   \n",
       "2963  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...   \n",
       "\n",
       "                                               html_url  \\\n",
       "0     https://github.com/huggingface/datasets/issues...   \n",
       "1     https://github.com/huggingface/datasets/issues...   \n",
       "2     https://github.com/huggingface/datasets/issues...   \n",
       "3     https://github.com/huggingface/datasets/issues...   \n",
       "4     https://github.com/huggingface/datasets/issues...   \n",
       "...                                                 ...   \n",
       "2959   https://github.com/huggingface/datasets/issues/2   \n",
       "2960   https://github.com/huggingface/datasets/issues/2   \n",
       "2961   https://github.com/huggingface/datasets/issues/2   \n",
       "2962   https://github.com/huggingface/datasets/issues/2   \n",
       "2963   https://github.com/huggingface/datasets/issues/2   \n",
       "\n",
       "                                               comments  \n",
       "0                       Cool, I think we can do both :)  \n",
       "1     @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...  \n",
       "2     Hi ! I guess the caching mechanism should have...  \n",
       "3     If it's easy enough to implement, then yes ple...  \n",
       "4     Well it can cause issue with anyone that updat...  \n",
       "...                                                 ...  \n",
       "2959  My first bug report ❤️\\r\\nLooking into this ri...  \n",
       "2960  Ok, there are some news, most good than bad :l...  \n",
       "2961  Ok great, so as discussed today, let's:\\r\\n- h...  \n",
       "2962  Good plan!\\r\\n\\r\\nYes I do use `builder_kwargs...  \n",
       "2963                                              Done!  \n",
       "\n",
       "[2964 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explode data to make a seperate row for each comment in the issue\n",
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1209583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9bbfc116af4aa08beae2050ccdd7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'body', 'html_url', 'comments'],\n",
       "    num_rows: 2898\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "\n",
    "# Filter out short comments\n",
    "comments_dataset = comments_dataset.filter(lambda x: len(x[\"comments\"]) > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e45a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create text embeddings\n",
    "checkpoint = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a51028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d5d66d0e1848df852c2c2bf7ee28ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c83a3e56e747899dbf80c69d35665c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    # Tokenize text\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move all tensors in encoded_input to GPU (i.e. \"input_ids\", \"token_type_ids\", \"attention_mask\")\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    \n",
    "    # Get model output (AKA embeddings)\n",
    "    model_output = model(**encoded_input)\n",
    "    \n",
    "    # We only need the embeddings from the CLS token \n",
    "    # which is the first token in the sequence and\n",
    "    # holds the meaning of the entire sequence\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "# Add text column (combined title, body, comments)\n",
    "text_dataset = comments_dataset.map(lambda x: {\n",
    "\t\"text\": x[\"title\"] + \"\\n\" + x[\"body\"] + \"\\n\" + x[\"comments\"]\n",
    "})\n",
    "\n",
    "embeddings_dataset = text_dataset.map(\n",
    "    lambda x: {\"embeddings\": [get_embeddings(o).detach().cpu().numpy()[0] for o in x[\"text\"]]},\n",
    "    batched=True,\n",
    "\tbatch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09dfcd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20da8af7ba9491f916a4e32642d1819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FAISS search\n",
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")\n",
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b521c71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "body",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "html_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comments",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "embeddings",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "scores",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "0f26fb37-93e8-4125-9342-0c69d855d88c",
       "rows": [
        [
         "4",
         "Discussion using datasets in offline mode",
         "`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/824",
         "Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\r\n\r\n@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?",
         "Discussion using datasets in offline mode\n`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n\nRequiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\r\n\r\n@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?",
         "[-0.47318094968795776, 0.24578382074832916, -0.012630809098482132, 0.14121420681476593, 0.2833554446697235, -0.14702121913433075, 0.6012275815010071, 0.012775982730090618, 0.26878103613853455, 0.13127796351909637, -0.024036630988121033, -0.02398541010916233, -0.024662071838974953, 0.40210649371147156, -0.0912403091788292, -0.10213790088891983, -0.17223328351974487, 0.03887735307216644, -0.1540050208568573, 0.13259465992450714, -0.1408108025789261, -0.1281195729970932, -0.3727090358734131, -0.028236012905836105, -0.16281409561634064, -0.18658731877803802, 0.009141966700553894, 0.14202120900154114, -0.15723170340061188, -0.2762964367866516, 0.2854101359844208, 0.18052466213703156, 0.2238955944776535, 0.47912484407424927, -0.00010423075582366437, 0.143766850233078, 0.05455521494150162, -0.013488792814314365, -0.20690283179283142, -0.33818691968917847, -0.19177496433258057, -0.3979363739490509, 0.23375803232192993, -0.40073737502098083, -0.21508470177650452, 0.03653720021247864, -0.07347273081541061, -0.3958377242088318, 0.3150738477706909, 0.30793818831443787, 0.2526886463165283, 0.6633721590042114, -0.14806096255779266, -0.32287004590034485, -0.17233116924762726, 0.010202423669397831, -0.23923420906066895, 0.3925468325614929, 0.06179112195968628, -0.07088661938905716, -0.11109884083271027, 0.11631783097982407, -0.11929061263799667, 0.011944344267249107, 0.35922935605049133, 0.09901522845029831, -0.04360618814826012, -0.09265778958797455, 0.1286545693874359, 0.45499011874198914, 0.3857675790786743, -0.22564664483070374, -0.32339122891426086, -0.4115026891231537, -0.12132839113473892, -0.39928504824638367, 0.07776372134685516, 0.09158197790384293, -0.05180191621184349, 0.2859633266925812, -0.45086073875427246, -0.27706092596054077, -0.36881232261657715, 0.08663364499807358, -0.3436936140060425, 0.1290588080883026, -0.22098089754581451, 0.05185969918966293, -0.05945218354463577, 0.17649854719638824, -0.1783764511346817, -0.28105098009109497, 0.13426481187343597, -0.131892129778862, -0.26535657048225403, -0.07102581113576889, 0.16602441668510437, 0.0489502027630806, 0.1325019747018814, 0.08357029408216476, 0.2456042319536209, 0.013486753217875957, 0.0630619153380394, 0.07287218421697617, 0.4931010603904724, 0.2701185345649719, -0.06640498340129852, -0.16545185446739197, 0.5901670455932617, 0.16802768409252167, 0.01540677435696125, -0.2984623312950134, 0.343881219625473, -0.24740858376026154, 0.05114375799894333, -0.004453452304005623, 0.39344069361686707, -0.29047682881355286, -0.07055893540382385, -0.02893909253180027, 0.26095694303512573, -0.042250026017427444, 0.23024773597717285, 0.2531338930130005, -0.20073562860488892, 0.023909155279397964, 0.0023877869825810194, 0.3707031309604645, -0.2583318054676056, -0.28777897357940674, -0.06917782872915268, -0.2514779567718506, -0.15245755016803741, 0.09091469645500183, 0.1942046880722046, 0.09884814918041229, 0.09316457062959671, -0.03527858108282089, 0.2799164354801178, 0.01931753195822239, 0.27842557430267334, -0.06133468821644783, -0.0014953874051570892, 0.3023599684238434, 0.11023347079753876, -0.06810878217220306, 0.1214374229311943, -0.186123326420784, 0.025751765817403793, 0.056542348116636276, -0.06533388793468475, -0.46220192313194275, 0.17617860436439514, 0.2197204977273941, -0.6732062101364136, -0.22239996492862701, -0.2730342149734497, 0.15556593239307404, -0.2369360476732254, -0.21249611675739288, 0.023586638271808624, 0.0184609517455101, -0.21189461648464203, -0.21170610189437866, 0.2878767251968384, 0.5173379778862, -0.4603883922100067, 0.012950203381478786, 0.04471082612872124, -0.2136072814464569, -0.20716486871242523, 0.10191818326711655, -0.4149196147918701, 0.6289389133453369, 0.010753807611763477, -0.38167500495910645, 0.559537947177887, -0.3048192262649536, -0.31402894854545593, 0.2726419270038605, -0.1605725735425949, -0.0045872763730585575, -0.05343218892812729, 0.10875438898801804, 0.46035295724868774, 0.0848672017455101, -0.09082350134849548, 0.2521170675754547, -0.12814529240131378, -0.013009965419769287, -0.26151078939437866, -0.3172561824321747, 0.19561423361301422, 0.3260178864002228, 0.3225422203540802, 0.1708083599805832, 0.2575957477092743, -0.18613705039024353, -0.008525182493031025, -0.1622692346572876, 0.12020473182201385, 0.042387455701828, 0.2255619615316391, 0.06719598919153214, -0.01023123413324356, -0.16534076631069183, -0.5502767562866211, 0.2796308398246765, -0.028709080070257187, 0.21882735192775726, -0.21580266952514648, -0.21077798306941986, -0.02763455919921398, -0.19524192810058594, -0.1810869425535202, -0.07955960184335709, 0.11711069196462631, -0.19782625138759613, 0.08592194318771362, 0.09828907251358032, -0.290071040391922, 0.11372992396354675, -0.011981319636106491, 0.02174258977174759, -0.1584361493587494, 0.30122289061546326, -0.04983896017074585, 0.08366227149963379, -0.15510967373847961, 0.05628940090537071, -0.054598722606897354, -0.16224323213100433, -0.09545866400003433, 0.5110428929328918, -0.023885929957032204, 0.05632632598280907, 0.23493926227092743, 0.5590959191322327, 0.36796677112579346, 0.004376022145152092, 0.49562910199165344, -0.050213005393743515, 0.321308970451355, -0.11147013306617737, -0.39841997623443604, 0.7357348203659058, -0.14405153691768646, 0.25747308135032654, 0.09899388998746872, -0.016456512734293938, 0.29107001423835754, -0.026504380628466606, -0.448882520198822, -0.02203684113919735, 0.018365582451224327, 0.014575710520148277, 0.21398736536502838, -0.03515465930104256, -0.30720821022987366, 0.007390991318970919, 0.36606860160827637, 0.06533291190862656, 0.150913804769516, -0.0016660802066326141, -0.0714009627699852, 0.06185619533061981, -0.08471042662858963, 0.09613891690969467, 0.5059415698051453, 0.2225106954574585, -0.0020844521932303905, 0.22226877510547638, -0.07521836459636688, -0.2918921411037445, 0.16224505007266998, 0.27840232849121094, 0.011779313907027245, 0.09482134878635406, 0.11065032333135605, -0.013544118963181973, -0.2606652081012726, -0.0851195678114891, 0.010414613410830498, 0.05213029682636261, -0.3171434998512268, 0.30235517024993896, -0.1921411007642746, 0.04467978700995445, 0.01166588719934225, 0.10387913137674332, -0.503385066986084, -0.19162964820861816, 0.1738937795162201, 0.08669839799404144, -0.27683913707733154, 0.16115067899227142, -0.0816926658153534, 0.530148446559906, -0.18624193966388702, -0.1722709685564041, 0.17858678102493286, -0.587223470211029, -0.061841897666454315, 0.09926629066467285, 0.3291292190551758, 0.03159284591674805, 0.46972161531448364, 0.11053143441677094, -0.10048867017030716, -0.22675028443336487, -0.25936561822891235, 0.07752905786037445, -0.044432383030653, 0.345569372177124, 0.07938052713871002, 0.04276946932077408, 0.07554012537002563, 0.29211121797561646, 0.4025549292564392, -0.2950232923030853, 0.25151899456977844, 0.1761237233877182, -0.12414395809173584, -0.04017336666584015, -0.37648484110832214, -0.4320443868637085, -0.38034945726394653, -0.41620081663131714, 0.20046290755271912, -0.04024699330329895, -0.04219407960772514, 0.2894973158836365, 0.18043404817581177, 0.06995631009340286, -0.07126184552907944, 0.0905177891254425, -0.1396663635969162, -0.4403934180736542, 0.22468464076519012, -0.15724800527095795, -0.5371837019920349, 0.20842115581035614, 0.1666211634874344, 0.1439088135957718, 0.27611464262008667, -0.5148003101348877, -0.3084290027618408, 0.059168457984924316, 0.3579506576061249, 0.1813366562128067, 0.22703933715820312, 0.39894482493400574, -0.10222463309764862, -0.02241155318915844, -0.08096662163734436, 0.0694311261177063, 0.2551831007003784, 0.04150112345814705, -0.014629045501351357, 0.04076270014047623, 0.3204084038734436, 0.012463468126952648, 0.4728301167488098, -0.06553752720355988, 0.062002040445804596, 0.24073722958564758, -0.09487786144018173, 0.6474265456199646, -0.2596339285373688, -0.1485426127910614, -0.07322894781827927, -0.046905484050512314, -0.3458384573459625, 0.2646626830101013, 0.02857278659939766, -0.01579633541405201, -0.5687810778617859, -0.016318801790475845, -0.24866268038749695, -0.44401389360427856, 0.18490788340568542, 0.2254667580127716, 0.3417711853981018, 0.14934124052524567, 0.2687123715877533, -0.28996050357818604, -0.037798017263412476, -0.055485282093286514, 0.2830597758293152, 0.06359421461820602, -0.11718087643384933, -0.24029448628425598, 0.0881894901394844, -0.3629257380962372, 0.2781027853488922, -0.20161467790603638, 0.17613312602043152, -0.0393388457596302, -0.054160166531801224, 0.055992916226387024, -0.07584438472986221, 0.47821807861328125, -0.38191941380500793, -0.116445392370224, 0.2526443898677826, -0.10346531122922897, -0.31540510058403015, 0.04721972718834877, -0.2072024643421173, -0.011010992340743542, 0.18806128203868866, 0.2789722681045532, -0.0935254693031311, -0.28632843494415283, -0.014317759312689304, 0.26077553629875183, -0.3398854732513428, -0.22574512660503387, -0.2240487039089203, -0.04733698070049286, -0.35305914282798767, 0.05012662708759308, -0.16760596632957458, -0.023787228390574455, -0.09883010387420654, 0.1071023941040039, -0.1596325784921646, 0.18465858697891235, -0.07270608097314835, 0.1555318385362625, 0.09379773586988449, -0.3015153408050537, 0.20171529054641724, 0.2891570031642914, 0.0554312989115715, 0.24392147362232208, 0.31305554509162903, -0.07576769590377808, -0.2507784068584442, 0.21849286556243896, -0.04491344466805458, 0.37438008189201355, 0.4474991261959076, -0.007546292617917061, -0.1503683179616928, 0.2359786182641983, 0.2909380793571472, -0.43079090118408203, -0.13248522579669952, 0.11488593369722366, 0.07833178341388702, -0.13668321073055267, -0.406774640083313, 0.24734430015087128, -0.004963520914316177, -0.051208723336458206, -0.0943637266755104, 0.1585390865802765, -0.23933495581150055, 0.31746163964271545, -0.1068653091788292, 0.7930576801300049, -0.06379719823598862, 0.244291290640831, 0.22235411405563354, -0.011960326693952084, 0.49882224202156067, -0.325268417596817, -0.022949080914258957, -0.03400176018476486, -0.17871084809303284, -0.03138192743062973, -0.07933221757411957, 0.4078507721424103, 0.05487211048603058, -0.13534681499004364, 0.5168306231498718, -0.03389408811926842, 0.38560062646865845, -0.28574270009994507, 0.12908253073692322, 0.05017952620983124, -0.26912713050842285, -0.5489072203636169, 0.20346568524837494, 0.09185373038053513, 0.07320915162563324, -0.22242620587348938, -0.2794913351535797, 0.06744261831045151, 0.0627632811665535, -0.026670917868614197, -0.18819163739681244, -0.032516513019800186, 0.07577595859766006, -0.13961373269557953, 0.05185887962579727, 0.0764455497264862, 0.12277158349752426, 0.40034055709838867, 0.12157931178808212, -0.4199985861778259, 0.05892254039645195, -0.39850175380706787, 0.15472722053527832, 0.022547351196408272, 0.1779077649116516, 0.18130876123905182, -0.1888165920972824, -0.24970415234565735, -0.008885608054697514, -0.10917118936777115, -0.10673082619905472, 0.4405537545681, -0.12163461744785309, 0.007391962688416243, -0.3226771652698517, 0.17373046278953552, 0.18594253063201904, 0.21774035692214966, -0.16453537344932556, 0.16309520602226257, 0.20322807133197784, 0.08970381319522858, 0.3784292936325073, -0.14693938195705414, -0.19359228014945984, -0.241688072681427, 0.2845246195793152, -0.11098363995552063, -0.1488572359085083, -0.07575993239879608, 0.22454619407653809, -0.18763622641563416, -0.3061800003051758, 0.22682902216911316, 0.07378678768873215, -0.28086501359939575, 0.2850721776485443, 0.3767935633659363, 0.23318889737129211, -0.11650453507900238, 0.2991284132003784, 0.06431978940963745, -0.2871188819408417, -0.1611163169145584, -0.3281608521938324, -0.38370177149772644, 0.24285311996936798, 0.41401779651641846, 0.16100788116455078, 0.07819455862045288, -0.039763301610946655, -0.006338054779917002, -0.1876194179058075, -0.40737950801849365, 0.021793711930513382, -0.05920334905385971, 0.06519895046949387, 0.26166486740112305, -0.21078836917877197, 0.38781097531318665, -0.20935465395450592, 0.05358780547976494, -0.18860487639904022, -0.41142022609710693, -0.27938228845596313, -0.037124261260032654, 0.18726423382759094, 0.034297894686460495, -0.32168442010879517, -0.048438750207424164, -0.4539428949356079, -0.39113596081733704, -0.14260241389274597, 0.2971314489841461, 0.45229867100715637, -0.140037402510643, 0.011962213553488255, -0.17946863174438477, 0.4436797499656677, -0.1515534669160843, 0.0933162197470665, 0.05384306237101555, 0.27223312854766846, 0.07948145270347595, 0.24185478687286377, -0.124284528195858, 0.06629180163145065, 0.005219592247158289, 0.1862124353647232, 0.2457854002714157, 0.15651607513427734, 0.48036468029022217, -0.1563785970211029, 0.14234144985675812, 0.3202059864997864, 0.5329152345657349, 0.1267692893743515, -0.05542465299367905, 0.1896502822637558, 0.07508888095617294, 0.2618720531463623, -0.4182620346546173, 0.055513445287942886, 0.055911798030138016, 0.06920772045850754, 0.34516412019729614, 0.16567936539649963, 0.4651868939399719, 0.15805335342884064, -0.012031146325170994, 0.044956982135772705, 0.37921467423439026, -0.08369647711515427, 0.23139533400535583, 0.13901585340499878, 0.054548267275094986, 0.14665241539478302, 0.09786364436149597, 0.36789149045944214, 0.027954580262303352, 0.2849605083465576, -0.3214753568172455, 0.17723938822746277, 0.04468119516968727, 0.10182744264602661, -0.1581350862979889, -0.4124341905117035, 0.2898588478565216, -0.04971444606781006, -0.05460119619965553, -0.18236380815505981, -0.25258874893188477, 0.18638139963150024, -0.43262988328933716, 0.01169374119490385, -0.21567213535308838, 0.06467040628194809, -0.07989908009767532, 0.010311554186046124, -0.3722074627876282, -0.3459406793117523, -0.021677488461136818, 0.11335430294275284, 0.033832475543022156, -0.012865054421126842, 0.04881085827946663, -0.08184895664453506, 0.09189504384994507, -0.21119017899036407, 0.14605502784252167, 0.058045268058776855, 0.07766140252351761, -0.030754027888178825, 0.031485822051763535, 0.2307012975215912, 0.02950032241642475, 0.1551375538110733, 0.49248188734054565, 0.29370173811912537, 0.03062821552157402, -0.21662628650665283, 0.1912286877632141, -0.02511400170624256, -0.09476242959499359, -0.11528361588716507, 0.11651831865310669, 0.057202938944101334, 0.1724911779165268, 0.31647977232933044, 0.15430869162082672, -0.20098768174648285, 0.3622320592403412, -0.2038223147392273, -0.06954973191022873, -0.4959082007408142, 0.3163032829761505, 0.17312265932559967, -0.14835397899150848, -0.040453195571899414, -0.2629607617855072, -0.16181543469429016, -0.24255554378032684, 0.3993489742279053, -0.018727554008364677, 0.14774738252162933, -0.19695760309696198, 0.10605733096599579, 0.027129197493195534, 0.44531187415122986, 0.15252888202667236, 0.25807705521583557, -0.27047860622406006, 0.22323979437351227, -0.7864072918891907, -0.15928606688976288, -0.07892975956201553, 0.34298422932624817, -0.017970575019717216, 0.10013223439455032, 0.11753738671541214, 0.4013325572013855, 0.05786038190126419, 0.06575602293014526, 0.14404159784317017, -0.17057473957538605, -0.2648235559463501, -0.08721107244491577, -0.1376919001340866, -0.059619732201099396, -0.02288045361638069, -0.2512318789958954, 0.11766733974218369, -0.24882198870182037, -0.01152590848505497, 0.010707668028771877, 0.1454162448644638, -0.11412221193313599, -0.06540808826684952, 0.43884792923927307, 0.10178735107183456, 0.36001983284950256, 0.25473982095718384, -0.0651024729013443, -0.10997193306684494, 0.013844858855009079, -0.09835454076528549, 0.04164398834109306, -0.0822126641869545, 0.02321673557162285, -0.17899549007415771, -0.18895168602466583, -0.19354243576526642, 0.13953398168087006, 0.09017349779605865, -0.08383339643478394, -0.20448502898216248, 0.02209462970495224, 0.00329687912017107, 0.13416831195354462, 0.0018572659464552999, 0.07419559359550476, -0.10655201226472855, 0.17237146198749542, -0.1570729911327362, 0.06434646993875504, 0.3616626560688019, -0.015810267999768257, -0.051304157823324203, -0.36193692684173584, 0.28459396958351135, 0.24544042348861694, 0.0160165186971426, -0.6187483072280884, 0.05920097976922989, 0.2603120803833008, -0.17047426104545593, -0.10797975212335587, 0.2965720593929291, -0.18775895237922668, -0.23386922478675842, -0.018963001668453217, 0.28340986371040344, -0.26333868503570557, -0.24726198613643646, 0.0018665847601369023, -0.12850797176361084]",
         "25.505016"
        ],
        [
         "3",
         "Discussion using datasets in offline mode",
         "`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/824",
         "The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\r\nYou can now use them offline\r\n```python\r\ndatasets = load_dataset('text', data_files=data_files)\r\n```\r\n\r\nWe'll do a new release soon",
         "Discussion using datasets in offline mode\n`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n\nThe local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\r\nYou can now use them offline\r\n```python\r\ndatasets = load_dataset('text', data_files=data_files)\r\n```\r\n\r\nWe'll do a new release soon",
         "[-0.44908520579338074, 0.2095070332288742, -0.05981921777129173, 0.12935017049312592, 0.2621927559375763, -0.13128575682640076, 0.5469649434089661, 0.09492632746696472, 0.31543827056884766, 0.22943148016929626, 0.051040660589933395, -0.0031587695702910423, 0.04794221371412277, 0.4036758840084076, -0.1032213419675827, -0.11917786300182343, -0.10968129336833954, 0.0806240439414978, -0.170927032828331, 0.09250310063362122, -0.1816653162240982, -0.08973146229982376, -0.3750767111778259, -0.02556263841688633, -0.10498400777578354, -0.17465969920158386, -0.10713951289653778, 0.15669572353363037, -0.21037450432777405, -0.33272048830986023, 0.24911323189735413, 0.11758773028850555, 0.16342730820178986, 0.4241682291030884, -9.998220048146322e-05, 0.042134299874305725, 0.10690491646528244, 0.037994347512722015, -0.22034716606140137, -0.41991594433784485, -0.1981758028268814, -0.4793078303337097, 0.24183820188045502, -0.4018023610115051, -0.1757831573486328, -0.12750133872032166, -0.05988450348377228, -0.4807882606983185, 0.2974688708782196, 0.34264591336250305, 0.2918210029602051, 0.5356078743934631, -0.08033845573663712, -0.281390905380249, -0.12166305631399155, -0.05861879512667656, -0.1778586059808731, 0.42528340220451355, 0.07335759699344635, -0.01237972266972065, -0.08388277143239975, 0.088844433426857, -0.1531631350517273, 0.09564132243394852, 0.3111797571182251, 0.07130653411149979, -0.09246546775102615, -0.15283311903476715, 0.20285534858703613, 0.3944040536880493, 0.46031829714775085, -0.3109174072742462, -0.2949755787849426, -0.3411385118961334, -0.04790077358484268, -0.43618789315223694, 0.12745609879493713, 0.14417366683483124, 0.009027406573295593, 0.23398277163505554, -0.363272488117218, -0.29756858944892883, -0.34185925126075745, 0.16949446499347687, -0.2816985845565796, 0.01810719631612301, -0.2516753077507019, 0.04136887565255165, -0.08324220776557922, 0.07528604567050934, -0.15515068173408508, -0.320526659488678, 0.09862120449542999, -0.07936308532953262, -0.18882839381694794, -0.01652996428310871, 0.15342731773853302, 0.008188928477466106, 0.12179774791002274, 0.07514936476945877, 0.1525360345840454, -0.006045398768037558, 0.09262832254171371, 0.06802230328321457, 0.4427141845226288, 0.31814658641815186, 0.018854208290576935, -0.2447725534439087, 0.5638324618339539, 0.06074818968772888, -0.004647275898605585, -0.2882612645626068, 0.32975849509239197, -0.2873264253139496, 0.03615562245249748, -0.01608704775571823, 0.3358898460865021, -0.2712709307670593, -0.09168356657028198, 0.060436733067035675, 0.14952248334884644, -0.08216547220945358, 0.15366840362548828, 0.2071380615234375, -0.2203720659017563, 0.029578227549791336, 0.0346219465136528, 0.3958794176578522, -0.2415044754743576, -0.23479627072811127, -0.10894816368818283, -0.10661912709474564, -0.1055871993303299, 0.0744708701968193, 0.18881645798683167, -0.013108670711517334, 0.12141093611717224, -0.0402393564581871, 0.21052229404449463, -0.03415481373667717, 0.27863723039627075, -0.1574505716562271, 0.04799852892756462, 0.2500379979610443, 0.1872590333223343, -0.10623395442962646, 0.05678074061870575, -0.2355709969997406, 0.03812050446867943, -0.003578917123377323, -0.04362648352980614, -0.40498724579811096, 0.09630904346704483, 0.2798323631286621, -0.5623058676719666, -0.16805486381053925, -0.254844069480896, 0.1297713965177536, -0.28230443596839905, -0.09712619334459305, 0.020675543695688248, -0.009743998758494854, -0.14495106041431427, -0.19830794632434845, 0.23908279836177826, 0.5462022423744202, -0.4452942907810211, 0.08768667280673981, 0.12178277969360352, -0.18718372285366058, -0.17599304020404816, 0.06420373916625977, -0.34309983253479004, 0.534393310546875, 0.05175808444619179, -0.2778848111629486, 0.5229827165603638, -0.3582608699798584, -0.23452143371105194, 0.2888381779193878, -0.16102811694145203, -0.14413931965827942, -0.06670589745044708, 0.04982958734035492, 0.3899843692779541, 0.11929743736982346, 0.00014352270227391273, 0.24310000240802765, -0.056448694318532944, 0.01956852898001671, -0.2536434233188629, -0.32352811098098755, 0.1407228410243988, 0.24940210580825806, 0.3065219819545746, 0.12399070709943771, 0.2792341113090515, -0.2578687071800232, -0.040598656982183456, -0.26234450936317444, 0.16544251143932343, 0.15791743993759155, 0.20993144810199738, 0.1023169532418251, -0.04836725443601608, -0.2059100866317749, -0.4760758876800537, 0.22526082396507263, 0.03369998559355736, 0.23208770155906677, -0.23021787405014038, -0.14966103434562683, -0.008372279815375805, -0.17970862984657288, -0.15976457297801971, -0.0425274595618248, 0.1691499799489975, -0.13886705040931702, 0.18496699631214142, 0.1760517805814743, -0.31897592544555664, 0.1439095139503479, -0.056265413761138916, 0.025987237691879272, -0.11554019153118134, 0.2796720862388611, -0.08094631880521774, 0.02319962903857231, 0.04264092817902565, 0.08386693894863129, -0.007074808236211538, -0.18599236011505127, -0.10737386345863342, 0.4920397400856018, -0.06001095846295357, 0.050862979143857956, 0.3152630925178528, 0.361380010843277, 0.35608765482902527, 0.03976289927959442, 0.5095083713531494, -0.12183874100446701, 0.3111967146396637, -0.07704389840364456, -0.33568817377090454, 0.6987401247024536, -0.1456831395626068, 0.20384849607944489, 0.09568314999341965, 0.002914729528129101, 0.31420081853866577, -0.0028085706289857626, -0.3935571610927582, -0.06533904373645782, 0.07977085560560226, -0.0444403700530529, 0.21489576995372772, -0.0678086206316948, -0.3889358639717102, -0.022998526692390442, 0.37562423944473267, -0.00017050481983460486, 0.18453139066696167, -0.00014205742627382278, 0.019494354724884033, 0.06274475157260895, -0.05969453975558281, 0.08411940187215805, 0.5141696333885193, 0.23863929510116577, 0.037967998534440994, 0.2522852420806885, -0.07030695676803589, -0.31523218750953674, 0.13953141868114471, 0.2154911905527115, 0.009246868081390858, 0.07897760719060898, 0.08997400850057602, -0.0033888304606080055, -0.30997100472450256, -0.05356886610388756, -0.029213111847639084, 0.08147195726633072, -0.2326790988445282, 0.245944082736969, -0.3154217004776001, -0.0520213358104229, 0.04224125295877457, 0.1119539737701416, -0.44370585680007935, -0.23124299943447113, 0.08682584762573242, 0.03582930564880371, -0.284227192401886, 0.14750051498413086, -0.08974266052246094, 0.45248112082481384, -0.204970583319664, -0.13876067101955414, 0.12793347239494324, -0.5095142722129822, -0.10333110392093658, 0.1338711977005005, 0.3391273617744446, 0.14795908331871033, 0.4992545545101166, 0.008694988675415516, -0.07728710025548935, -0.2907158136367798, -0.18749713897705078, 0.0661986693739891, -0.08808484673500061, 0.31891924142837524, 0.06686320900917053, 0.04812246933579445, -0.04875685274600983, 0.22543616592884064, 0.4193474054336548, -0.30619704723358154, 0.17071449756622314, 0.08558611571788788, -0.052098970860242844, -0.15533879399299622, -0.3459175229072571, -0.4872054159641266, -0.41300591826438904, -0.4035470485687256, 0.2704857885837555, 0.02673843502998352, -0.029249010607600212, 0.1243099644780159, 0.15803834795951843, 0.10010389238595963, -0.017610948532819748, 0.2050553560256958, -0.21885642409324646, -0.4933132827281952, 0.12404114753007889, -0.23167584836483002, -0.5781070590019226, 0.2055889368057251, 0.1968107670545578, 0.18133218586444855, 0.23949718475341797, -0.48664820194244385, -0.3747950494289398, 0.04225495830178261, 0.39738568663597107, 0.18707506358623505, 0.31775081157684326, 0.366006463766098, -0.024661770090460777, -0.07663138210773468, -0.07843238860368729, 0.02465914748609066, 0.2000739574432373, 0.09790173918008804, -0.12540161609649658, 0.07418999820947647, 0.31684941053390503, -0.05953498184680939, 0.431792676448822, -0.011108567006886005, 0.049614328891038895, 0.26420047879219055, -0.11141473799943924, 0.6812874674797058, -0.2694683074951172, -0.14404229819774628, 0.013289475813508034, -0.05878763645887375, -0.2512549161911011, 0.21060743927955627, 0.01576525717973709, -0.03625711798667908, -0.6542542576789856, -0.009702837094664574, -0.4030838906764984, -0.3856436610221863, 0.08408999443054199, 0.13159188628196716, 0.32492688298225403, 0.12207826972007751, 0.2325800359249115, -0.28019651770591736, 0.007007401902228594, -0.07424620538949966, 0.24773119390010834, 0.06175926327705383, -0.09989432990550995, -0.26814356446266174, 0.009503146633505821, -0.3775522708892822, 0.3366495668888092, -0.12583878636360168, 0.2580755054950714, -0.07711391150951385, -0.03052177093923092, 0.007170767057687044, -0.12932038307189941, 0.35944095253944397, -0.3762831389904022, -0.1614653319120407, 0.2891311049461365, -0.09685558080673218, -0.27328503131866455, 0.08448909968137741, -0.1972348690032959, 0.04960980266332626, 0.14479830861091614, 0.2760511636734009, -0.08429268002510071, -0.3076002895832062, 0.061061326414346695, 0.35047999024391174, -0.24975556135177612, -0.1882382333278656, -0.1717916876077652, -0.15453435480594635, -0.3299389183521271, 0.0528557188808918, -0.2317122519016266, 0.0491180345416069, -0.08698075264692307, 0.11360476911067963, -0.19496701657772064, 0.1558360904455185, -0.06426027417182922, 0.09747349470853806, 0.07337220013141632, -0.2716398537158966, 0.2272786647081375, 0.2307698130607605, 0.11610417068004608, 0.3351897895336151, 0.33777347207069397, -0.1233740821480751, -0.24711428582668304, 0.18110884726047516, -0.05647524073719978, 0.36204248666763306, 0.4465494751930237, -0.07298950850963593, -0.09751884639263153, 0.15274707973003387, 0.27581173181533813, -0.4200821816921234, -0.14561881124973297, 0.205525740981102, 0.17815177142620087, -0.06869249790906906, -0.37601494789123535, 0.24424150586128235, 0.1129923164844513, -0.053321827203035355, 0.015025592409074306, 0.12303374707698822, -0.18846207857131958, 0.22140295803546906, -0.059303365647792816, 0.7759789824485779, -0.06803908199071884, 0.15732692182064056, 0.22853076457977295, -0.04602701589465141, 0.480792373418808, -0.31898435950279236, 0.01727907918393612, -0.04013900086283684, -0.19411927461624146, 0.003633217653259635, -0.10702309757471085, 0.4337177574634552, -0.041575923562049866, -0.07330742478370667, 0.5114348530769348, 0.04939230531454086, 0.3181031346321106, -0.3153342008590698, 0.2438233494758606, -0.0738772451877594, -0.20791108906269073, -0.51924067735672, 0.24451316893100739, 0.11848004162311554, 0.08330311626195908, -0.21407559514045715, -0.24249276518821716, 0.0730278491973877, 0.0449502132833004, -0.0016657899832352996, -0.168387770652771, 0.02441302128136158, 0.09009180963039398, -0.16062773764133453, -0.026859525591135025, -0.010152050293982029, 0.10606607049703598, 0.4375321567058563, 0.1366603523492813, -0.32997775077819824, 0.007785526569932699, -0.3949720859527588, 0.15776784718036652, 0.026052335277199745, 0.20851382613182068, 0.20681333541870117, -0.21949417889118195, -0.2341146320104599, 0.019462354481220245, -0.11119107156991959, -0.04316318407654762, 0.37168020009994507, -0.09794808179140091, -0.07603996992111206, -0.2462717741727829, 0.18239791691303253, 0.2029300183057785, 0.1959368884563446, -0.11741670966148376, 0.20119591057300568, 0.2146024852991104, 0.12221911549568176, 0.3525826930999756, -0.137693852186203, -0.20146749913692474, -0.29379037022590637, 0.3205306828022003, -0.08456265181303024, -0.17774850130081177, -0.07223301380872726, 0.21065755188465118, -0.23534436523914337, -0.3138156235218048, 0.22931021451950073, 0.04631678760051727, -0.11343999952077866, 0.2572039067745209, 0.33158889412879944, 0.24621276557445526, -0.14655038714408875, 0.26909372210502625, 0.07185428589582443, -0.1856536567211151, -0.07777969539165497, -0.32748153805732727, -0.34138888120651245, 0.25607404112815857, 0.45137640833854675, 0.1558241993188858, 0.11476074904203415, -0.05527380108833313, 0.08663864433765411, -0.1971708983182907, -0.44087520241737366, 0.06798561662435532, -0.10292553901672363, 0.0660289004445076, 0.24939361214637756, -0.1730809062719345, 0.37666189670562744, -0.19028496742248535, 0.10507398843765259, -0.171529620885849, -0.33767426013946533, -0.31054311990737915, 0.040069982409477234, 0.14592796564102173, -0.012359009124338627, -0.29672908782958984, -0.025278648361563683, -0.5543741583824158, -0.3203270137310028, -0.13491666316986084, 0.20853649079799652, 0.41518905758857727, -0.1913514882326126, 0.02760280668735504, -0.18674983084201813, 0.3740745484828949, -0.03890533372759819, 0.0819929838180542, 0.03259176388382912, 0.30306321382522583, -0.0030837226659059525, 0.24330036342144012, -0.029512159526348114, 0.04305749014019966, 0.02539442665874958, 0.10726051032543182, 0.08517158776521683, 0.13141927123069763, 0.4154767394065857, -0.22441565990447998, 0.10768750309944153, 0.29655247926712036, 0.548306405544281, 0.21454206109046936, -0.08642926812171936, 0.18920102715492249, 0.16145502030849457, 0.3001292645931244, -0.3635794222354889, 0.003281757701188326, 0.02434028871357441, 0.13692599534988403, 0.36104026436805725, 0.06150847673416138, 0.41261544823646545, 0.022325364872813225, -0.07993048429489136, 0.022095613181591034, 0.32850995659828186, -0.10955975204706192, 0.2245308756828308, 0.12351561337709427, 0.06354068219661713, 0.10752560198307037, 0.0822858214378357, 0.47520050406455994, 0.016164563596248627, 0.3380523920059204, -0.27611008286476135, 0.19018812477588654, 0.0358075350522995, 0.16101045906543732, -0.19965973496437073, -0.38033196330070496, 0.3735842704772949, 0.012612645514309406, -0.09095717966556549, -0.22750258445739746, -0.22773531079292297, 0.2517726421356201, -0.4506409466266632, -0.06101137027144432, -0.17156605422496796, 0.13332051038742065, -0.12181324511766434, 0.02488069050014019, -0.39790618419647217, -0.3176393508911133, 0.015624374151229858, 0.12472616881132126, 0.07117877900600433, -0.04802757874131203, 0.0062537225894629955, -0.042227696627378464, 0.07728675752878189, -0.18694980442523956, 0.19665588438510895, -0.044592272490262985, 0.09621073305606842, -0.010903085581958294, 0.02591830864548683, 0.220855250954628, -0.005506064742803574, 0.05313070863485336, 0.4975874125957489, 0.3380313217639923, 0.04152796044945717, -0.23035010695457458, 0.14561580121517181, -0.052200593054294586, -0.14743554592132568, -0.10493658483028412, 0.17912279069423676, 0.037331607192754745, 0.11591558903455734, 0.37948840856552124, 0.2081170380115509, -0.22560662031173706, 0.3958668112754822, -0.11642235517501831, -0.11571706086397171, -0.42895519733428955, 0.23676259815692902, 0.2151343822479248, -0.1547243446111679, -0.041834548115730286, -0.30537325143814087, -0.19344459474086761, -0.16616252064704895, 0.4117177426815033, -0.0005006838473491371, 0.13323137164115906, -0.2520880103111267, 0.14978741109371185, 0.10072019696235657, 0.45058876276016235, 0.18467363715171814, 0.24237418174743652, -0.23920337855815887, 0.07929526269435883, -0.6985386610031128, -0.1670779287815094, -0.1281266063451767, 0.32637691497802734, -0.07358379662036896, 0.1057542935013771, 0.11156905442476273, 0.4124389588832855, -0.006307756528258324, 0.1760130375623703, 0.059697989374399185, -0.2131684571504593, -0.31564149260520935, -0.0660589337348938, -0.1222003921866417, -0.12911835312843323, 0.007462688721716404, -0.22769267857074738, 0.14308054745197296, -0.33697646856307983, 0.03819803521037102, 0.0014560060808435082, 0.22137871384620667, -0.17708617448806763, -0.1268341988325119, 0.4185492992401123, 0.10407986491918564, 0.3364284038543701, 0.1634533554315567, -0.08263374865055084, -0.046754710376262665, -0.0029865603428333998, -0.06484009325504303, 0.19435328245162964, -0.07710418850183487, -0.03476523980498314, -0.1695019155740738, -0.17485743761062622, -0.16514210402965546, 0.19062605500221252, 0.040540825575590134, 0.02373824454843998, -0.1428375244140625, -0.0490613617002964, 0.058735791593790054, 0.061685316264629364, 0.052843961864709854, 0.08251367509365082, -0.11660251766443253, 0.2132478952407837, -0.1673649400472641, 0.01565101183950901, 0.3196122646331787, -0.0693761333823204, 0.00991101749241352, -0.3681209683418274, 0.28379663825035095, 0.250424861907959, -0.03425562381744385, -0.46022549271583557, 0.15612639486789703, 0.2961423397064209, -0.18321220576763153, -0.15913566946983337, 0.2970786988735199, -0.21991178393363953, -0.18535834550857544, -0.05066053941845894, 0.18979878723621368, -0.22448930144309998, -0.25366660952568054, 0.013835308142006397, -0.15920893847942352]",
         "24.55553"
        ],
        [
         "2",
         "Discussion using datasets in offline mode",
         "`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/824",
         "I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\r\n\r\nLet me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \r\n\r\nI already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\r\n\r\n----------\r\n\r\n> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\r\n\r\nIndeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\r\nFor example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\r\n```python\r\nload_dataset(\"./my_dataset\")\r\n```\r\nand the dataset script will generate your dataset once and for all.\r\n\r\n----------\r\n\r\nAbout I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\r\ncf #1724 ",
         "Discussion using datasets in offline mode\n`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n\nI opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\r\n\r\nLet me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \r\n\r\nI already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\r\n\r\n----------\r\n\r\n> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\r\n\r\nIndeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\r\nFor example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\r\n```python\r\nload_dataset(\"./my_dataset\")\r\n```\r\nand the dataset script will generate your dataset once and for all.\r\n\r\n----------\r\n\r\nAbout I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\r\ncf #1724 ",
         "[-0.47164809703826904, 0.2902272641658783, -0.04767198860645294, 0.13344208896160126, 0.2106890082359314, -0.2122250497341156, 0.5858259201049805, 0.05341639742255211, 0.28334107995033264, 0.18411290645599365, 0.031059805303812027, 0.03263862803578377, 0.0011813620803877711, 0.33332574367523193, -0.07214409112930298, -0.05854186415672302, -0.1905425637960434, -0.005104868207126856, -0.16585811972618103, 0.0827917829155922, -0.08703489601612091, -0.17246869206428528, -0.4247153699398041, -0.11953160166740417, -0.06756014376878738, -0.16881519556045532, -0.02421172335743904, 0.2116723656654358, -0.1926143318414688, -0.4453730285167694, 0.2666606307029724, 0.20002925395965576, 0.15709368884563446, 0.422985315322876, -0.00010433600982651114, 0.05918848142027855, 0.0797744020819664, 0.012605804018676281, -0.2829338610172272, -0.35053759813308716, -0.23761780560016632, -0.4724888801574707, 0.24548931419849396, -0.4179917871952057, -0.17655570805072784, -0.03197115659713745, -0.11496420949697495, -0.4964653253555298, 0.2549799084663391, 0.33320048451423645, 0.25219547748565674, 0.5795009136199951, -0.17738166451454163, -0.25776582956314087, -0.093070849776268, -0.045575790107250214, -0.21700289845466614, 0.41301771998405457, 0.14518100023269653, -0.011996750719845295, -0.09174735099077225, 0.04279641807079315, -0.20566576719284058, 0.05553530901670456, 0.3426395356655121, 0.06921125203371048, -0.10128755867481232, -0.07574755698442459, 0.1654936671257019, 0.4096829891204834, 0.4384515881538391, -0.278817743062973, -0.23378755152225494, -0.3661539554595947, -0.05999082326889038, -0.415805846452713, 0.057996299117803574, 0.08426307886838913, -0.050141531974077225, 0.2703288793563843, -0.41784971952438354, -0.2782534658908844, -0.363169401884079, 0.18717209994792938, -0.29582542181015015, 0.08009763807058334, -0.21904611587524414, 0.057191286236047745, -0.0345732606947422, 0.12171023339033127, -0.1603945791721344, -0.2874974310398102, 0.12319684773683548, -0.055233608931303024, -0.24581922590732574, -0.04832232743501663, 0.17336606979370117, 0.026207970455288887, 0.14559617638587952, 0.10752960294485092, 0.16631299257278442, 0.055469904094934464, 0.006958374287933111, 0.05163701996207237, 0.4685065746307373, 0.21164576709270477, 0.011062650941312313, -0.20973332226276398, 0.5494055151939392, 0.1489725112915039, 0.009058672934770584, -0.23407568037509918, 0.2657729983329773, -0.2524653971195221, 0.09119803458452225, 0.07008790224790573, 0.37652087211608887, -0.3214930593967438, -0.022985287010669708, 0.035314127802848816, 0.15973259508609772, -0.06210384890437126, 0.16163760423660278, 0.17477960884571075, -0.28472453355789185, 0.012940475717186928, -0.029524730518460274, 0.360969215631485, -0.277133971452713, -0.3468533754348755, -0.03897637128829956, -0.21186435222625732, -0.17805108428001404, 0.0920449048280716, 0.24551859498023987, 0.03093670681118965, 0.051060836762189865, -0.01477278582751751, 0.24244040250778198, -0.03684663772583008, 0.30183932185173035, -0.08095026016235352, 0.06594803184270859, 0.21205894649028778, 0.11961734294891357, -0.07656988501548767, 0.12485961616039276, -0.156015545129776, 0.006711690220981836, 0.06666768342256546, -0.0514347180724144, -0.4386141896247864, 0.23742879927158356, 0.2237865924835205, -0.6274787783622742, -0.1871650665998459, -0.19690218567848206, 0.07779024541378021, -0.30176863074302673, -0.17387612164020538, 0.02149554342031479, 0.07558473944664001, -0.16909335553646088, -0.23825474083423615, 0.2987489700317383, 0.6010069251060486, -0.5180040001869202, 0.08190575987100601, 0.04580340161919594, -0.18163956701755524, -0.18896904587745667, 0.02832123078405857, -0.39451226592063904, 0.5864567756652832, 0.003940659109503031, -0.3889358341693878, 0.5678292512893677, -0.30959412455558777, -0.30935195088386536, 0.28757986426353455, -0.18025138974189758, -0.059503454715013504, -0.03820360079407692, 0.11968886107206345, 0.4773573875427246, 0.08094943314790726, -0.007814125157892704, 0.2666751444339752, -0.07702412456274033, -0.00954200979322195, -0.2956812381744385, -0.2896501123905182, 0.09854305535554886, 0.2965259253978729, 0.2704697251319885, 0.1280689537525177, 0.34545832872390747, -0.23669256269931793, 0.027660099789500237, -0.13620029389858246, 0.13635103404521942, 0.09909966588020325, 0.23302766680717468, 0.1239425390958786, -0.06952321529388428, -0.13211774826049805, -0.5115547180175781, 0.3242626190185547, 0.039750825613737106, 0.16213595867156982, -0.20566017925739288, -0.153311088681221, -0.040165361016988754, -0.216920405626297, -0.1385665237903595, -0.03778631240129471, 0.1234515905380249, -0.08959077298641205, 0.08697434514760971, 0.17630697786808014, -0.2977674901485443, 0.13516117632389069, -0.01829402893781662, -0.005147712770849466, -0.15002185106277466, 0.3400147557258606, -0.03143706172704697, 0.05650637298822403, -0.1015019416809082, 0.0904150977730751, 0.01230020821094513, -0.17409959435462952, -0.10554558783769608, 0.48379212617874146, 0.04152840003371239, 0.11835478246212006, 0.2707458436489105, 0.4232816994190216, 0.3186858594417572, 0.0245528481900692, 0.5603365898132324, -0.1271902620792389, 0.3274112045764923, -0.11851813644170761, -0.34176701307296753, 0.7027892470359802, -0.17834393680095673, 0.272815465927124, 0.11503125727176666, -0.03400534763932228, 0.20388177037239075, -0.09402220696210861, -0.4044407308101654, -0.09520289301872253, 0.06635507196187973, 0.0030854810029268265, 0.1993396133184433, 0.01687702350318432, -0.2931217849254608, 0.05046212300658226, 0.33741772174835205, 0.013531599193811417, 0.13642315566539764, -0.060463786125183105, -0.017177371308207512, 0.11304952204227448, -0.02549026720225811, 0.13787993788719177, 0.5132737159729004, 0.23082058131694794, 0.043037232011556625, 0.3077917695045471, 0.03528771921992302, -0.2593405246734619, 0.11889748275279999, 0.1687576025724411, -0.018508922308683395, 0.12123826146125793, 0.09633904695510864, -0.011798874475061893, -0.1399933099746704, -0.03193850442767143, 0.003812602022662759, 0.05903532728552818, -0.3146701157093048, 0.29025962948799133, -0.35456034541130066, -0.027913345023989677, 0.013022050261497498, 0.1309276670217514, -0.4807069003582001, -0.21028168499469757, 0.11700571328401566, 0.056921929121017456, -0.2447338104248047, 0.17129236459732056, -0.08770693838596344, 0.4514833390712738, -0.26934513449668884, -0.26088786125183105, 0.24277828633785248, -0.5937601327896118, -0.0655437558889389, 0.08480043709278107, 0.34014102816581726, 0.18221251666545868, 0.5658236145973206, 0.059399280697107315, -0.14113403856754303, -0.27103981375694275, -0.14470207691192627, 0.08929383754730225, -0.011607530526816845, 0.31521543860435486, 0.10839823633432388, 0.020331164821982384, 0.027070913463830948, 0.40024954080581665, 0.3858611285686493, -0.2979997396469116, 0.1902173012495041, 0.12723973393440247, -0.09454520046710968, -0.03227195143699646, -0.33833280205726624, -0.4963890612125397, -0.39923328161239624, -0.46776121854782104, 0.1700815111398697, -0.052777353674173355, 0.01070215180516243, 0.22977836430072784, 0.20387543737888336, 0.1109456717967987, -0.07051600515842438, 0.11048652976751328, -0.17224201560020447, -0.46132969856262207, 0.1450522094964981, -0.1792818158864975, -0.49374449253082275, 0.25376269221305847, 0.21924853324890137, 0.19170072674751282, 0.3427467942237854, -0.5899962782859802, -0.3177875876426697, 0.10433157533407211, 0.39143136143684387, 0.10402370244264603, 0.26793283224105835, 0.39332032203674316, -0.07943177968263626, -0.05855543911457062, -0.090384840965271, 0.0574980266392231, 0.20767448842525482, 0.05640185996890068, -0.10910288989543915, 0.14945587515830994, 0.2685927152633667, 0.01676233671605587, 0.5026165843009949, -0.07983443886041641, 0.10981667786836624, 0.2587999701499939, -0.11742191016674042, 0.7056583762168884, -0.1825232207775116, -0.16715535521507263, -0.04138531908392906, 0.014881533570587635, -0.32470953464508057, 0.17284083366394043, -0.017981991171836853, -0.01590685173869133, -0.6336121559143066, -0.07981642335653305, -0.24845226109027863, -0.45507821440696716, 0.18346771597862244, 0.15974074602127075, 0.32477840781211853, 0.1058613583445549, 0.21936215460300446, -0.2707967758178711, -0.025644708424806595, -0.0919201597571373, 0.26326337456703186, -0.014952282421290874, -0.07749012857675552, -0.2507243752479553, 0.13699989020824432, -0.3115708529949188, 0.29557254910469055, -0.13525080680847168, 0.2162645310163498, -0.04390702396631241, -0.031342435628175735, 0.08267325162887573, -0.03918434679508209, 0.29914215207099915, -0.40821823477745056, -0.09436909109354019, 0.3293496370315552, -0.021643666550517082, -0.3531903922557831, 0.07643704116344452, -0.17220939695835114, 0.001942998729646206, 0.15991565585136414, 0.1581936478614807, -0.06794283539056778, -0.3483630120754242, -0.006039553787559271, 0.35970842838287354, -0.2774302363395691, -0.21776695549488068, -0.2001277506351471, -0.0423307903110981, -0.42750778794288635, -0.005213992204517126, -0.25458937883377075, -0.09843573719263077, -0.08035686612129211, 0.1182885617017746, -0.15449242293834686, 0.21917633712291718, -0.13801021873950958, 0.07399655133485794, 0.11023329198360443, -0.3203083574771881, 0.22734123468399048, 0.29776814579963684, 0.055301059037446976, 0.3309391736984253, 0.35064777731895447, -0.1181175708770752, -0.14827962219715118, 0.20862434804439545, -0.043347276747226715, 0.4113818109035492, 0.48546552658081055, -0.00959391612559557, -0.08460164070129395, 0.14470809698104858, 0.28923526406288147, -0.40379026532173157, -0.16152170300483704, 0.1078801080584526, 0.059298496693372726, -0.10008542239665985, -0.4673405587673187, 0.18348947167396545, -0.04053424671292305, -0.07383877784013748, -0.0035255462862551212, 0.12941695749759674, -0.18516236543655396, 0.24160151183605194, -0.09198204427957535, 0.8122491240501404, 0.026364944875240326, 0.22195416688919067, 0.17398056387901306, -0.007890915498137474, 0.47341322898864746, -0.3028861880302429, -0.045583952218294144, -0.060265351086854935, -0.04364381358027458, 0.018221039324998856, -0.07604072988033295, 0.4410827159881592, 0.010689853690564632, -0.20546954870224, 0.4934930205345154, -0.0749412328004837, 0.2824133038520813, -0.33753037452697754, 0.10760150849819183, 0.02022414468228817, -0.22647900879383087, -0.5203982591629028, 0.2028241604566574, 0.10815141350030899, 0.1028197631239891, -0.18519316613674164, -0.2435837835073471, 0.03425271809101105, 0.1493041217327118, 0.00956664513796568, -0.18984968960285187, -0.05260547623038292, 0.030810117721557617, -0.20590335130691528, 0.0560678169131279, 0.0032343219500035048, 0.04502616822719574, 0.3648868501186371, 0.1468627005815506, -0.33987095952033997, 0.013886780478060246, -0.35569095611572266, 0.1691974252462387, 0.03607769310474396, 0.17526204884052277, 0.1566658318042755, -0.19859498739242554, -0.33446481823921204, 0.0011107660830020905, -0.11166401207447052, -0.025551915168762207, 0.4310033321380615, -0.054841529577970505, 0.09423397481441498, -0.28058913350105286, 0.17674873769283295, 0.2303113043308258, 0.2608221173286438, -0.09356134384870529, 0.17464551329612732, 0.21108199656009674, 0.0750604048371315, 0.3884125053882599, -0.12536145746707916, -0.2212529480457306, -0.2144705355167389, 0.3347359001636505, -0.12474638968706131, -0.15533871948719025, -0.0833655297756195, 0.2226354479789734, -0.1695810854434967, -0.3365005850791931, 0.3025306761264801, -0.005420953966677189, -0.22300845384597778, 0.19864065945148468, 0.35047176480293274, 0.29631707072257996, -0.02601812593638897, 0.2046554982662201, 0.010253200307488441, -0.1880027949810028, -0.16794084012508392, -0.2753778398036957, -0.31788429617881775, 0.16600710153579712, 0.4533556401729584, 0.1563892513513565, 0.047357212752103806, -0.09701032936573029, -0.014806973747909069, -0.2212061733007431, -0.42416587471961975, 0.06605008244514465, -0.0683547705411911, 0.08642970770597458, 0.28571823239326477, -0.1539192646741867, 0.30052468180656433, -0.2875232696533203, 0.07204852253198624, -0.21196866035461426, -0.4271269142627716, -0.2759726047515869, -0.05654674768447876, 0.17803406715393066, -0.03753342851996422, -0.3255496323108673, -0.02707800269126892, -0.5326867699623108, -0.312399297952652, -0.12824073433876038, 0.27275189757347107, 0.4409954249858856, -0.1296406388282776, 0.005306201055645943, -0.2117784023284912, 0.5039592981338501, -0.14663320779800415, 0.16231229901313782, 0.029629327356815338, 0.3317891061306, 0.008961674757301807, 0.20383206009864807, -0.09911542385816574, 0.0761970579624176, 0.08180315047502518, 0.15743780136108398, 0.21227334439754486, 0.12287351489067078, 0.44521549344062805, -0.14625108242034912, 0.15636754035949707, 0.24989208579063416, 0.5009362101554871, 0.16290190815925598, -0.07047199457883835, 0.121659055352211, 0.20804786682128906, 0.263833612203598, -0.33409351110458374, 0.006575312465429306, 0.07387255877256393, 0.14300955832004547, 0.34018272161483765, 0.14444907009601593, 0.4526940584182739, 0.09535852819681168, 0.005178947001695633, 0.012638217769563198, 0.3435201346874237, -0.07534995675086975, 0.22372213006019592, 0.07389146834611893, 0.046985726803541183, 0.059665244072675705, -0.037816084921360016, 0.3901122808456421, 0.047124601900577545, 0.27826228737831116, -0.3217484951019287, 0.22000178694725037, 0.0860397070646286, 0.15097448229789734, -0.24488918483257294, -0.47545719146728516, 0.41145703196525574, 0.07126782089471817, -0.10157586634159088, -0.1877400279045105, -0.28618907928466797, 0.22444461286067963, -0.37016430497169495, -0.05365986377000809, -0.07750001549720764, 0.08453787118196487, -0.06990154832601547, 0.08859515190124512, -0.38110995292663574, -0.35103297233581543, -0.07359430193901062, 0.11835092306137085, 0.0528443306684494, -0.024111242964863777, 0.027261966839432716, -0.03470591828227043, 0.09389667958021164, -0.3102189004421234, 0.1447046846151352, 0.040217477828264236, 0.09825889021158218, -0.030762072652578354, 0.031080877408385277, 0.18606781959533691, 0.02186816744506359, 0.14004552364349365, 0.43245476484298706, 0.28055712580680847, 0.08853333443403244, -0.18219876289367676, 0.17896811664104462, 0.00964664202183485, -0.06145734339952469, -0.1308431476354599, 0.07283755391836166, 0.0065970453433692455, 0.11478954553604126, 0.32942360639572144, 0.15862935781478882, -0.19990165531635284, 0.35883843898773193, -0.13050350546836853, -0.22178809344768524, -0.49707189202308655, 0.23960034549236298, 0.23929299414157867, -0.12374549359083176, -0.058407701551914215, -0.26564890146255493, -0.25000667572021484, -0.2632139325141907, 0.4473288953304291, -0.009919374249875546, 0.14950764179229736, -0.23250676691532135, 0.11047729849815369, -0.00396709656342864, 0.48722440004348755, 0.13599254190921783, 0.26269009709358215, -0.2467203587293625, 0.1591036468744278, -0.8487070202827454, -0.10102493315935135, -0.13505753874778748, 0.26252323389053345, -0.006098833400756121, 0.07039369642734528, 0.1294202357530594, 0.49746713042259216, 0.07324076443910599, 0.1348397582769394, 0.19894088804721832, -0.17261433601379395, -0.31127265095710754, -0.15946777164936066, -0.07587838917970657, -0.0788232833147049, -0.06832202523946762, -0.24027439951896667, 0.06171315163373947, -0.34754109382629395, -0.0013598324730992317, 0.02756172977387905, 0.10042748600244522, -0.08791477978229523, -0.012023880146443844, 0.5188180804252625, 0.028614286333322525, 0.34539127349853516, 0.3027183413505554, -0.1201029047369957, -0.10799453407526016, 0.07161689549684525, -0.023257968947291374, 0.05834650620818138, -0.03804274648427963, -0.051774270832538605, -0.1984517127275467, -0.21233871579170227, -0.2314855456352234, 0.22407960891723633, 0.0803287997841835, 0.023162266239523888, -0.07774920761585236, -0.05085020139813423, 0.09004464745521545, 0.1569569706916809, -0.06310082972049713, 0.034296341240406036, -0.09619159251451492, 0.24042794108390808, -0.2528861165046692, 0.08049584180116653, 0.33428066968917847, 0.017955288290977478, 0.017515292391180992, -0.3761807680130005, 0.34296709299087524, 0.23217856884002686, 0.017630478367209435, -0.5226221084594727, 0.09299908578395844, 0.31454628705978394, -0.15647217631340027, -0.04333699122071266, 0.3258461356163025, -0.25415095686912537, -0.1967097371816635, 0.011406022123992443, 0.2230950891971588, -0.27748236060142517, -0.22277282178401947, 0.06501627713441849, -0.11797735840082169]",
         "24.148987"
        ],
        [
         "1",
         "Discussion using datasets in offline mode",
         "`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/824",
         "> here is my way to load a dataset offline, but it **requires** an online machine\n> \n> 1. (online machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_dataset(...)\n> \n> data.save_to_disk(/YOUR/DATASET/DIR)\n> \n> ```\n> \n> 2. copy the dir from online to the offline machine\n> \n> 3. (offline machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n> \n> ```\n> \n> \n> \n> HTH.\n\n",
         "Discussion using datasets in offline mode\n`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n\n> here is my way to load a dataset offline, but it **requires** an online machine\n> \n> 1. (online machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_dataset(...)\n> \n> data.save_to_disk(/YOUR/DATASET/DIR)\n> \n> ```\n> \n> 2. copy the dir from online to the offline machine\n> \n> 3. (offline machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n> \n> ```\n> \n> \n> \n> HTH.\n\n",
         "[-0.4992601275444031, 0.22699803113937378, -0.03246923163533211, 0.1418721228837967, 0.23695072531700134, -0.10291733592748642, 0.5442942380905151, 0.07441116124391556, 0.275363028049469, 0.24428823590278625, -0.00883389450609684, -0.06653949618339539, 0.028085896745324135, 0.37562260031700134, -0.09889979660511017, -0.04195534065365791, -0.16233326494693756, 0.056354623287916183, -0.18303605914115906, 0.10765429586172104, -0.16337786614894867, -0.0869513675570488, -0.433300256729126, -0.06250779330730438, -0.015938423573970795, -0.16068170964717865, -0.07989776879549026, 0.2115720808506012, -0.17174974083900452, -0.284518837928772, 0.24363406002521515, 0.16650104522705078, 0.19889163970947266, 0.4731353521347046, -0.00010347333591198549, 0.037023890763521194, 0.011921290308237076, 0.010489378124475479, -0.22773189842700958, -0.3849579393863678, -0.22434748709201813, -0.514648973941803, 0.16380608081817627, -0.45466411113739014, -0.20701086521148682, -0.03795715793967247, -0.06889118999242783, -0.5084753632545471, 0.30955979228019714, 0.35181209444999695, 0.2570660412311554, 0.5904159545898438, -0.12827081978321075, -0.2792428135871887, -0.1558482050895691, -0.1522628217935562, -0.155697762966156, 0.4283839464187622, -0.017295414581894875, 0.001368585741147399, -0.022243965417146683, 0.10412988811731339, -0.17523492872714996, 0.034660376608371735, 0.32222139835357666, 0.06727868318557739, -0.10973989218473434, -0.08459015935659409, 0.22063317894935608, 0.3967529237270355, 0.4247179329395294, -0.28843435645103455, -0.25191909074783325, -0.33961620926856995, -0.007588682696223259, -0.4678497016429901, 0.09609856456518173, 0.1565520018339157, 0.021752774715423584, 0.2291809469461441, -0.39443182945251465, -0.30916258692741394, -0.3443600833415985, 0.15244723856449127, -0.32422229647636414, 0.002370665315538645, -0.26090380549430847, 0.07101435959339142, -0.06112496182322502, 0.09298722445964813, -0.197912335395813, -0.2828267812728882, 0.06065140664577484, -0.07426091283559799, -0.2604791224002838, 0.014129758812487125, 0.12018026411533356, 0.06370105594396591, 0.14024204015731812, 0.05883759260177612, 0.2486957460641861, -0.009835027158260345, 0.09796891361474991, 0.051038842648267746, 0.44227761030197144, 0.29570233821868896, 0.007448864169418812, -0.18953922390937805, 0.5416594743728638, 0.09937040507793427, -0.007566854823380709, -0.2980295419692993, 0.28207987546920776, -0.3101066052913666, 0.13364864885807037, 0.007375683635473251, 0.3609675168991089, -0.2954883277416229, -0.16744408011436462, -0.012540464289486408, 0.14054597914218903, -0.042656056582927704, 0.18352529406547546, 0.2483586221933365, -0.28737354278564453, 0.02999025397002697, 0.06376826763153076, 0.3856900632381439, -0.2997948229312897, -0.25466543436050415, -0.0616324283182621, -0.13591280579566956, -0.12915778160095215, 0.07275398075580597, 0.18642577528953552, 0.008041813038289547, 0.0445537269115448, -0.03349096700549126, 0.2402402013540268, -0.0691184550523758, 0.3054918944835663, -0.10393402725458145, 0.07850288599729538, 0.23368456959724426, 0.2520386278629303, -0.09576693922281265, 0.06249643862247467, -0.19691194593906403, 0.03824615105986595, 0.04105864092707634, -0.10746696591377258, -0.359640896320343, 0.1783694624900818, 0.27268528938293457, -0.6226015090942383, -0.17106537520885468, -0.30796369910240173, 0.05818531662225723, -0.3064323961734772, -0.19844305515289307, -0.015349581837654114, -0.0460299476981163, -0.19359515607357025, -0.24053096771240234, 0.33769491314888, 0.5559729337692261, -0.3982942998409271, 0.09689789265394211, 0.07916382700204849, -0.17718902230262756, -0.18636676669120789, 0.01955353654921055, -0.3082571029663086, 0.5199756026268005, -0.018785420805215836, -0.29418736696243286, 0.564875066280365, -0.3655901551246643, -0.27014273405075073, 0.2469819188117981, -0.18696478009223938, -0.11204149574041367, -0.09344162046909332, 0.09128867834806442, 0.4447420835494995, 0.08528806269168854, -0.09592305123806, 0.2809373736381531, -0.045862164348363876, -0.0005546778556890786, -0.22116267681121826, -0.2394605427980423, 0.15443791449069977, 0.2881149649620056, 0.27238160371780396, 0.09690294414758682, 0.30917084217071533, -0.19527238607406616, 0.028024576604366302, -0.2787551283836365, 0.13509629666805267, 0.1634456217288971, 0.1533001959323883, 0.09200994670391083, -0.043815791606903076, -0.1929367333650589, -0.4884025752544403, 0.2647080421447754, 0.0410783588886261, 0.16328826546669006, -0.20425909757614136, -0.1388627290725708, -0.021051740273833275, -0.18907134234905243, -0.16223375499248505, 0.013333267532289028, 0.13976101577281952, -0.10027668625116348, 0.16114330291748047, 0.11377903074026108, -0.2563471794128418, 0.17995229363441467, 0.020155789330601692, 0.007480001542717218, -0.13851122558116913, 0.26936689019203186, -0.05760365352034569, 0.06681857258081436, -0.0368148572742939, 0.09325654804706573, 0.04240242391824722, -0.19589261710643768, -0.1061081737279892, 0.5290290713310242, -0.030969103798270226, 0.04685305058956146, 0.26507341861724854, 0.38039088249206543, 0.3248947262763977, -0.011337267234921455, 0.5266871452331543, -0.0440165139734745, 0.3241904675960541, -0.0970291718840599, -0.34095051884651184, 0.7555660605430603, -0.15243227779865265, 0.21270206570625305, 0.11122913658618927, 0.011510811746120453, 0.26559147238731384, 0.002492331201210618, -0.40446925163269043, -0.02973901480436325, 0.10320593416690826, 0.0016841520555317402, 0.19528348743915558, -0.08575377613306046, -0.3268732726573944, -0.0021087590139359236, 0.34893906116485596, -0.013892320916056633, 0.17515349388122559, -0.06133606284856796, -0.03181490674614906, 0.12014266103506088, -0.07345383614301682, 0.08592979609966278, 0.521972119808197, 0.2544082999229431, 0.0612386092543602, 0.23070238530635834, -0.04147021844983101, -0.25591006875038147, 0.13679473102092743, 0.2250439077615738, 0.034236181527376175, 0.07031279802322388, 0.08283457905054092, -0.03545493260025978, -0.2182726114988327, -0.05210096761584282, -0.041350286453962326, 0.016514915972948074, -0.2833809554576874, 0.28712889552116394, -0.24826297163963318, -0.0287768617272377, 0.0571509450674057, 0.13270080089569092, -0.44999629259109497, -0.19225071370601654, 0.07581085711717606, 0.08342628180980682, -0.2347823828458786, 0.13678625226020813, -0.1147691085934639, 0.41578546166419983, -0.2417861372232437, -0.21620942652225494, 0.2557031512260437, -0.4892721474170685, -0.12540850043296814, 0.11391950398683548, 0.29047656059265137, 0.12110724300146103, 0.5174888372421265, 0.018219321966171265, -0.09722797572612762, -0.33553507924079895, -0.16106517612934113, 0.03558218106627464, -0.07128263264894485, 0.28297892212867737, 0.09335041046142578, 0.053073156625032425, -0.06716349720954895, 0.3253515660762787, 0.39373141527175903, -0.3271918296813965, 0.19344578683376312, 0.17926442623138428, -0.04971756413578987, -0.08718906342983246, -0.35299497842788696, -0.4282160699367523, -0.4320696294307709, -0.3919803202152252, 0.19262002408504486, 0.01324391271919012, -0.05743659660220146, 0.18983705341815948, 0.14822158217430115, 0.11652818322181702, 0.005847228225320578, 0.18890005350112915, -0.1883193403482437, -0.4760991036891937, 0.16884909570217133, -0.1892995685338974, -0.6016698479652405, 0.2806210219860077, 0.2295091152191162, 0.16576312482357025, 0.358272522687912, -0.44481581449508667, -0.37814977765083313, 0.07329194992780685, 0.440105676651001, 0.11024243384599686, 0.33426526188850403, 0.35336410999298096, -0.059358879923820496, -0.04304688051342964, -0.09738732129335403, 0.07812046259641647, 0.19092883169651031, 0.05161779373884201, -0.1234370619058609, 0.03781817480921745, 0.26290908455848694, -0.028633423149585724, 0.40237340331077576, -0.03312085568904877, 0.09678024053573608, 0.22856254875659943, -0.15662604570388794, 0.7003898024559021, -0.18450887501239777, -0.14877091348171234, -0.01886690966784954, -0.05123775824904442, -0.3176400363445282, 0.24771809577941895, 0.02027062140405178, 0.003444742877036333, -0.6579137444496155, -0.030888745561242104, -0.36408835649490356, -0.41016659140586853, 0.1023244708776474, 0.12379638850688934, 0.36120104789733887, 0.11311563104391098, 0.28462061285972595, -0.2655317187309265, -0.08067990839481354, -0.08379016816616058, 0.2637239992618561, 0.07059052586555481, -0.08213328570127487, -0.23685690760612488, 0.03539062291383743, -0.3795093297958374, 0.37882182002067566, -0.1252356618642807, 0.19933485984802246, -0.04949464648962021, -0.020566169172525406, -0.015860749408602715, -0.1601906567811966, 0.3575388789176941, -0.3219276964664459, -0.018434908241033554, 0.3051733672618866, -0.0723779946565628, -0.3048909902572632, 0.07500392943620682, -0.218546524643898, -0.018709072843194008, 0.18014948070049286, 0.16952012479305267, -0.11954507231712341, -0.30047667026519775, 0.01672636903822422, 0.43107354640960693, -0.28221261501312256, -0.1958409547805786, -0.22202467918395996, -0.15081866085529327, -0.3353959918022156, 0.011330798268318176, -0.19399437308311462, 0.07243090867996216, -0.10425866395235062, 0.1709119826555252, -0.25316888093948364, 0.16413122415542603, -0.01689092628657818, 0.11212849617004395, 0.12095918506383896, -0.2463967353105545, 0.24926602840423584, 0.2544647753238678, 0.10754436999559402, 0.31589606404304504, 0.3369898796081543, -0.05784446746110916, -0.28811752796173096, 0.1851859837770462, -0.021639615297317505, 0.354046493768692, 0.3804875314235687, -0.10842529684305191, -0.12979084253311157, 0.15628349781036377, 0.27723103761672974, -0.42217954993247986, -0.10582294315099716, 0.15206663310527802, 0.18993531167507172, -0.1318843513727188, -0.3830699324607849, 0.24210809171199799, 0.10584716498851776, -0.04281158000230789, -0.027909889817237854, 0.14056654274463654, -0.21628811955451965, 0.16577540338039398, -0.12047450989484787, 0.746323823928833, -0.08539478480815887, 0.22619514167308807, 0.2716042101383209, -0.010599088855087757, 0.4589008688926697, -0.21827082335948944, -0.017234019935131073, -0.029943592846393585, -0.25807905197143555, -0.0018668727716431022, -0.08380193263292313, 0.4105694591999054, 0.012097237631678581, -0.10150577872991562, 0.5569252967834473, 0.002353810239583254, 0.2621751129627228, -0.3156479299068451, 0.20682883262634277, -0.026268193498253822, -0.22416001558303833, -0.5295374989509583, 0.22278498113155365, 0.11169318854808807, 0.0908094272017479, -0.19659674167633057, -0.22969141602516174, 0.08920556306838989, 0.0733070895075798, 0.02098306640982628, -0.08815831691026688, -0.01472658384591341, 0.0910443365573883, -0.2471253126859665, -0.01132481824606657, 0.0023112038616091013, 0.11687834560871124, 0.36628925800323486, 0.1540875881910324, -0.30085864663124084, 0.040769968181848526, -0.3916715979576111, 0.17588822543621063, 0.043192680925130844, 0.2353018820285797, 0.17323698103427887, -0.23165880143642426, -0.22949795424938202, 0.07112269103527069, -0.09329880028963089, -0.022983374074101448, 0.44661352038383484, -0.05345890298485756, -0.0299995094537735, -0.22703272104263306, 0.17019732296466827, 0.1871999353170395, 0.1742008626461029, -0.12725897133350372, 0.16901274025440216, 0.24697433412075043, 0.06463587284088135, 0.33248475193977356, -0.11856259405612946, -0.2234364151954651, -0.23192928731441498, 0.33238667249679565, -0.1481754034757614, -0.21637843549251556, -0.0340101532638073, 0.21277499198913574, -0.19113387167453766, -0.30224207043647766, 0.2718290388584137, 0.005080138333141804, -0.1282939314842224, 0.23439939320087433, 0.38825827836990356, 0.37624281644821167, -0.11516568809747696, 0.27027785778045654, 0.03198745846748352, -0.24230632185935974, -0.0731426402926445, -0.3703365921974182, -0.2799696922302246, 0.20772677659988403, 0.4752342700958252, 0.12351345270872116, 0.14776809513568878, -0.08008366823196411, 0.04716597497463226, -0.2568991482257843, -0.41482269763946533, 0.04279755800962448, -0.05798890441656113, 0.049095626920461655, 0.25880786776542664, -0.1258111149072647, 0.36290276050567627, -0.21997465193271637, 0.09443126618862152, -0.23529593646526337, -0.3346165418624878, -0.2838246822357178, -0.006551886443048716, 0.17334482073783875, -0.044216278940439224, -0.32150429487228394, -0.009209181182086468, -0.5947596430778503, -0.3334682583808899, -0.15249447524547577, 0.2498432844877243, 0.4137350618839264, -0.1798788458108902, -0.02680150419473648, -0.19731801748275757, 0.43178361654281616, -0.17644132673740387, 0.09491526335477829, 0.06300605833530426, 0.28773975372314453, -0.03423914313316345, 0.27980780601501465, -0.05040222033858299, 0.038233283907175064, -0.029126251116394997, 0.10646600276231766, 0.1639818549156189, 0.07929295301437378, 0.3910473883152008, -0.2540487051010132, 0.14306358993053436, 0.25833603739738464, 0.5726849436759949, 0.19121384620666504, -0.05390661209821701, 0.18754595518112183, 0.0983947291970253, 0.271558940410614, -0.3790971040725708, 0.060038011521101, -0.0015360129764303565, 0.10761107504367828, 0.4134906828403473, 0.08459675312042236, 0.32913580536842346, 0.11033905297517776, -0.09633927792310715, -0.00930819846689701, 0.33048006892204285, -0.11191628128290176, 0.2232806384563446, 0.1307685822248459, 0.043918561190366745, 0.14179550111293793, 0.03111920692026615, 0.4378817677497864, -0.007967829704284668, 0.32399359345436096, -0.33903607726097107, 0.23513735830783844, 0.1155502051115036, 0.11277183890342712, -0.2816658914089203, -0.4408525228500366, 0.28789710998535156, 0.030565697699785233, -0.048554349690675735, -0.18208639323711395, -0.24641045928001404, 0.18192560970783234, -0.5260382294654846, -0.08454267680644989, -0.1453719437122345, 0.09366638213396072, -0.07735245674848557, 0.06396112591028214, -0.40598350763320923, -0.30864468216896057, -0.03175346553325653, 0.12277428060770035, 0.0772300511598587, -0.0736873596906662, -0.0024294457398355007, 0.01450976263731718, 0.1101318970322609, -0.20595158636569977, 0.13572469353675842, 0.05308615416288376, 0.06743227690458298, -0.03144587203860283, 0.031584762036800385, 0.2038814276456833, 0.07596318423748016, 0.1162949874997139, 0.4558700621128082, 0.3250945210456848, 0.027958879247307777, -0.1732877939939499, 0.17912086844444275, -0.017408886924386024, -0.12570112943649292, -0.10850207507610321, 0.09677736461162567, 0.027115345001220703, 0.1316801756620407, 0.36574801802635193, 0.1895967423915863, -0.2076842188835144, 0.34449267387390137, -0.07613754272460938, -0.15048278868198395, -0.45688068866729736, 0.2505193054676056, 0.18464668095111847, -0.11699195206165314, -0.10645521432161331, -0.2874948978424072, -0.1889008730649948, -0.10830489546060562, 0.37259748578071594, 0.01947627402842045, 0.11053317040205002, -0.19144263863563538, 0.12801173329353333, 0.10187994688749313, 0.43914657831192017, 0.17338897287845612, 0.2939247488975525, -0.2876732349395752, 0.11599361151456833, -0.8045647144317627, -0.14938189089298248, -0.12592154741287231, 0.2573934495449066, -0.051809173077344894, 0.10918913036584854, 0.0875609889626503, 0.437338650226593, 0.07446430623531342, 0.1585407555103302, 0.1727095991373062, -0.18016521632671356, -0.3003026247024536, -0.1347002536058426, -0.062439434230327606, -0.10021425783634186, -0.03152811527252197, -0.31961867213249207, 0.09199731051921844, -0.36339306831359863, 0.02291492559015751, 0.06731802225112915, 0.20422449707984924, -0.20613275468349457, -0.12975479662418365, 0.4416781961917877, 0.0433068722486496, 0.3950856029987335, 0.14411050081253052, -0.11225063353776932, -0.05289555341005325, 0.03797508031129837, -0.06393860280513763, 0.1701035350561142, -0.036108557134866714, -0.032888513058423996, -0.11447662115097046, -0.14136379957199097, -0.1595001518726349, 0.21476691961288452, 0.06328552216291428, -0.011129392310976982, -0.10949485749006271, -0.03615781292319298, 0.07759091258049011, 0.13357815146446228, 0.0008277703891508281, 0.04459936544299126, -0.14516198635101318, 0.21255788207054138, -0.2044009119272232, 0.03149844706058502, 0.33396270871162415, -0.03406418114900589, 0.018112890422344208, -0.3719015121459961, 0.3834597170352936, 0.17423349618911743, -0.006817786954343319, -0.49317052960395813, 0.2285158634185791, 0.3217989206314087, -0.16859857738018036, -0.14056412875652313, 0.305350661277771, -0.25994908809661865, -0.18955975770950317, 0.00045287786633707583, 0.26143398880958557, -0.25957760214805603, -0.29740551114082336, 0.05708596110343933, -0.14739368855953217]",
         "22.894005"
        ],
        [
         "0",
         "Discussion using datasets in offline mode",
         "`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n",
         "https://github.com/huggingface/datasets/issues/824",
         "here is my way to load a dataset offline, but it **requires** an online machine\r\n1. (online machine)\r\n```\r\nimport datasets\r\ndata = datasets.load_dataset(...)\r\ndata.save_to_disk(/YOUR/DATASET/DIR)\r\n```\r\n2. copy the dir from online to the offline machine\r\n3. (offline machine)\r\n```\r\nimport datasets\r\ndata = datasets.load_from_disk(/SAVED/DATA/DIR)\r\n```\r\n\r\nHTH.",
         "Discussion using datasets in offline mode\n`datasets.load_dataset(\"csv\", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.\r\n\r\nI create this ticket to discuss a bit and gather what you have in mind or other propositions.\r\n\r\nHere are some points to open discussion:\r\n- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.\r\n- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(\"MY_PATH/csv.py\", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.\r\n- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least \"freeze\" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.\r\n \r\nWDYT? (thks)\r\n\r\n\nhere is my way to load a dataset offline, but it **requires** an online machine\r\n1. (online machine)\r\n```\r\nimport datasets\r\ndata = datasets.load_dataset(...)\r\ndata.save_to_disk(/YOUR/DATASET/DIR)\r\n```\r\n2. copy the dir from online to the offline machine\r\n3. (offline machine)\r\n```\r\nimport datasets\r\ndata = datasets.load_from_disk(/SAVED/DATA/DIR)\r\n```\r\n\r\nHTH.",
         "[-0.49025753140449524, 0.22889599204063416, -0.03322082385420799, 0.13887320458889008, 0.23637248575687408, -0.08911523222923279, 0.5481941103935242, 0.06737265735864639, 0.2955958843231201, 0.2450512945652008, -0.010174185037612915, -0.06949268281459808, 0.02762574702501297, 0.38272836804389954, -0.10571837425231934, -0.021846644580364227, -0.15303950011730194, 0.053661495447158813, -0.17722824215888977, 0.08962273597717285, -0.16522905230522156, -0.09213337302207947, -0.43372344970703125, -0.0704062283039093, -0.018133241683244705, -0.15042583644390106, -0.08321225643157959, 0.2194388061761856, -0.1661250740289688, -0.2807520031929016, 0.23882104456424713, 0.163314089179039, 0.2291969358921051, 0.4750613272190094, -0.00010338118590880185, 0.026317141950130463, 0.008208809420466423, -0.001379936933517456, -0.22697371244430542, -0.40258845686912537, -0.21580463647842407, -0.5285043716430664, 0.1796485185623169, -0.4550705552101135, -0.20320403575897217, -0.05820455774664879, -0.07459700852632523, -0.49292734265327454, 0.3164803683757782, 0.3498673141002655, 0.25937768816947937, 0.5757412314414978, -0.1358262449502945, -0.27544477581977844, -0.16067932546138763, -0.15148751437664032, -0.1493048220872879, 0.43661585450172424, -0.014577280730009079, -0.008213034830987453, -0.002577077131718397, 0.09523333609104156, -0.19615651667118073, 0.041385866701602936, 0.31634071469306946, 0.05868138000369072, -0.10160994529724121, -0.10181795805692673, 0.2198021411895752, 0.393084853887558, 0.42616188526153564, -0.2737804353237152, -0.24700316786766052, -0.3390044569969177, -0.0023412539158016443, -0.45674702525138855, 0.07884853333234787, 0.18367138504981995, 0.02422848343849182, 0.22488416731357574, -0.3797212243080139, -0.3182392418384552, -0.33132484555244446, 0.16381365060806274, -0.3214631676673889, 0.01116943359375, -0.2536792755126953, 0.06930167227983475, -0.06124476343393326, 0.09133492410182953, -0.17682068049907684, -0.28515323996543884, 0.07391130924224854, -0.061946868896484375, -0.259450227022171, 0.02685556560754776, 0.11837919801473618, 0.06844944506883621, 0.13120520114898682, 0.0652054026722908, 0.23980161547660828, -0.01881815493106842, 0.0791068971157074, 0.049734581261873245, 0.4407639801502228, 0.2978355884552002, 0.007579691242426634, -0.17687177658081055, 0.52901291847229, 0.09103353321552277, -0.02419479750096798, -0.298907071352005, 0.25659680366516113, -0.3073078691959381, 0.1321207731962204, 0.013797827996313572, 0.3654094338417053, -0.3048083186149597, -0.15878844261169434, -0.013998952694237232, 0.13926081359386444, -0.042463041841983795, 0.16893228888511658, 0.2522922158241272, -0.27162498235702515, 0.04514198750257492, 0.06977430731058121, 0.3969353139400482, -0.29897844791412354, -0.26576560735702515, -0.05894829332828522, -0.12591956555843353, -0.11281128972768784, 0.06377102434635162, 0.17899082601070404, 6.016126644681208e-05, 0.052043478935956955, -0.041127998381853104, 0.24216538667678833, -0.07350905239582062, 0.29814714193344116, -0.09517253935337067, 0.08237814903259277, 0.23530831933021545, 0.27505868673324585, -0.08741816878318787, 0.07691621780395508, -0.20032168924808502, 0.037915538996458054, 0.038790639489889145, -0.14262734353542328, -0.35825714468955994, 0.17065675556659698, 0.2765567898750305, -0.6289306879043579, -0.17536665499210358, -0.32162588834762573, 0.03675992041826248, -0.31611377000808716, -0.19427327811717987, -0.030854033306241035, -0.06167130917310715, -0.1780046820640564, -0.24536627531051636, 0.3240107297897339, 0.5668389797210693, -0.383969783782959, 0.10502681136131287, 0.08440620452165604, -0.16043423116207123, -0.17819255590438843, 0.023399027064442635, -0.3148404657840729, 0.523370087146759, -0.013056292198598385, -0.3010377883911133, 0.5681837797164917, -0.3550918698310852, -0.26516038179397583, 0.2487460970878601, -0.19176085293293, -0.10596559941768646, -0.10386128723621368, 0.09011474996805191, 0.44007647037506104, 0.07504848390817642, -0.08439385145902634, 0.28901076316833496, -0.035171136260032654, 0.00046764567377977073, -0.21861092746257782, -0.2369784116744995, 0.16731786727905273, 0.2837589681148529, 0.25556206703186035, 0.11290042102336884, 0.30474260449409485, -0.19725698232650757, 0.039163000881671906, -0.28212112188339233, 0.11916597932577133, 0.16548573970794678, 0.1697200983762741, 0.10239680856466293, -0.04420793056488037, -0.18281495571136475, -0.48932284116744995, 0.27184173464775085, 0.04788379743695259, 0.15719570219516754, -0.1992020159959793, -0.1379159390926361, -0.020389018580317497, -0.183131143450737, -0.17696978151798248, 0.02728227898478508, 0.14106407761573792, -0.11078741401433945, 0.17246270179748535, 0.1293688267469406, -0.25989830493927, 0.17829249799251556, 0.010788321495056152, 0.0024608317762613297, -0.1300736963748932, 0.29033994674682617, -0.04923708736896515, 0.06460001319646835, -0.02892928197979927, 0.08814696222543716, 0.058180924504995346, -0.19372235238552094, -0.11858335137367249, 0.5259700417518616, -0.03009079024195671, 0.06162949651479721, 0.2687849700450897, 0.38185641169548035, 0.30646389722824097, -0.012045410461723804, 0.5338502526283264, -0.04272610321640968, 0.3334376811981201, -0.0925104171037674, -0.3228546679019928, 0.7542522549629211, -0.1525755077600479, 0.21712739765644073, 0.1025259792804718, 0.018323883414268494, 0.25997957587242126, 0.0012787692248821259, -0.4040777087211609, -0.035952311009168625, 0.11844827979803085, 0.0022457698360085487, 0.1811373084783554, -0.0850738063454628, -0.3064593970775604, -0.015291319228708744, 0.33124661445617676, -0.01883956976234913, 0.17788520455360413, -0.06607446819543839, -0.030485041439533234, 0.13060492277145386, -0.0831732451915741, 0.08876874297857285, 0.5169287919998169, 0.25069645047187805, 0.051957257091999054, 0.22982555627822876, -0.03137839958071709, -0.25621455907821655, 0.13256654143333435, 0.23213565349578857, 0.0492413304746151, 0.06843370199203491, 0.10048214346170425, -0.023727037012577057, -0.2063811719417572, -0.05208294466137886, -0.05471893027424812, 0.023271804675459862, -0.2868105471134186, 0.2843013107776642, -0.2516608238220215, -0.01631532981991768, 0.04705161601305008, 0.11078917980194092, -0.448403000831604, -0.200299933552742, 0.06472034007310867, 0.07769657671451569, -0.2318822294473648, 0.1336180716753006, -0.12687160074710846, 0.39075222611427307, -0.2403128445148468, -0.23885786533355713, 0.2622438669204712, -0.4872460961341858, -0.12277626991271973, 0.1072285994887352, 0.3064814507961273, 0.131101593375206, 0.5087966918945312, 0.021420041099190712, -0.09715385735034943, -0.330574095249176, -0.15721817314624786, 0.015584703534841537, -0.06593610346317291, 0.2778906524181366, 0.0809209793806076, 0.05781939998269081, -0.045380350202322006, 0.3248307704925537, 0.387504905462265, -0.33749276399612427, 0.20367646217346191, 0.18218089640140533, -0.05648786574602127, -0.0917128473520279, -0.34320753812789917, -0.4313037693500519, -0.44154611229896545, -0.39765897393226624, 0.17419353127479553, 0.03497724607586861, -0.05292617529630661, 0.18278957903385162, 0.15197280049324036, 0.10292713344097137, 0.016677072271704674, 0.18532288074493408, -0.18284305930137634, -0.46745172142982483, 0.1784859597682953, -0.18080560863018036, -0.603835940361023, 0.2909511923789978, 0.231156587600708, 0.15280160307884216, 0.3490709960460663, -0.4436717927455902, -0.3765626549720764, 0.06810653954744339, 0.4156067669391632, 0.10857711732387543, 0.3436666429042816, 0.34344905614852905, -0.070281021296978, -0.03828602284193039, -0.09237951040267944, 0.09284019470214844, 0.17694687843322754, 0.05897199735045433, -0.13427136838436127, 0.03360636532306671, 0.25433552265167236, -0.029425067827105522, 0.3790816366672516, -0.03754815086722374, 0.09186223894357681, 0.23645862936973572, -0.1578764021396637, 0.6929190158843994, -0.18278200924396515, -0.15251706540584564, -0.03947843611240387, -0.05371118709445, -0.32495397329330444, 0.24910147488117218, 0.024523265659809113, 0.0068166907876729965, -0.6647502779960632, -0.04201798141002655, -0.3744180500507355, -0.4079519510269165, 0.09886421263217926, 0.12205903977155685, 0.3661222457885742, 0.11318635940551758, 0.27275583148002625, -0.2575813829898834, -0.08721696585416794, -0.08682448416948318, 0.25321105122566223, 0.0775294303894043, -0.07182230055332184, -0.24946142733097076, 0.024586930871009827, -0.364916056394577, 0.38078972697257996, -0.14542917907238007, 0.1998324692249298, -0.034633535891771317, -0.023042455315589905, -0.012695053592324257, -0.1682795286178589, 0.34386077523231506, -0.316940039396286, -0.012938409112393856, 0.2967361807823181, -0.06794029474258423, -0.3190101683139801, 0.06565112620592117, -0.21159285306930542, -0.02457975223660469, 0.18760710954666138, 0.16614565253257751, -0.10674972087144852, -0.3034741282463074, 0.012663677334785461, 0.44400420784950256, -0.29345792531967163, -0.19761565327644348, -0.2340572327375412, -0.16527114808559418, -0.34483078122138977, -0.007720455061644316, -0.20222271978855133, 0.06967046856880188, -0.12043342739343643, 0.18467147648334503, -0.2544876039028168, 0.16877156496047974, -0.0028256464283913374, 0.10414904356002808, 0.12790967524051666, -0.23415161669254303, 0.25221145153045654, 0.2547444999217987, 0.10652103275060654, 0.32324540615081787, 0.34943103790283203, -0.05301897972822189, -0.28627386689186096, 0.16815932095050812, -0.016019051894545555, 0.3586544394493103, 0.3766716718673706, -0.0966339036822319, -0.1404377669095993, 0.13751918077468872, 0.27444028854370117, -0.41917139291763306, -0.10130854696035385, 0.14703457057476044, 0.1796005815267563, -0.11906620860099792, -0.3914579749107361, 0.26411324739456177, 0.10821402072906494, -0.025815283879637718, -0.028694940730929375, 0.13354435563087463, -0.2077488750219345, 0.19106806814670563, -0.11176501959562302, 0.7465305328369141, -0.0877658799290657, 0.22428621351718903, 0.25353294610977173, 7.72406201576814e-05, 0.44187232851982117, -0.21456454694271088, -0.03927897289395332, -0.041099876165390015, -0.25300002098083496, -0.004827496130019426, -0.08334273844957352, 0.41198691725730896, 0.004282220732420683, -0.08891209214925766, 0.5612855553627014, -0.02958664484322071, 0.25405213236808777, -0.3048314154148102, 0.2157217562198639, -0.023164372891187668, -0.22042718529701233, -0.5347031354904175, 0.223642498254776, 0.11935305595397949, 0.1017455980181694, -0.1956087350845337, -0.22880618274211884, 0.0823684111237526, 0.07364948093891144, 0.0393216498196125, -0.07162731140851974, -0.0051057301461696625, 0.09715690463781357, -0.251177579164505, -0.01161945890635252, -0.007524427492171526, 0.14217005670070648, 0.35666120052337646, 0.14511723816394806, -0.3017606735229492, 0.023752011358737946, -0.3742334544658661, 0.1623164266347885, 0.048931822180747986, 0.23895572125911713, 0.17136813700199127, -0.22986821830272675, -0.22958247363567352, 0.07708224654197693, -0.09000764787197113, -0.009264971129596233, 0.4244615137577057, -0.048909906297922134, -0.009292048402130604, -0.23481987416744232, 0.14177152514457703, 0.18841029703617096, 0.19095605611801147, -0.12081723660230637, 0.16989926993846893, 0.2535119354724884, 0.053100407123565674, 0.3266041874885559, -0.11337970197200775, -0.22215597331523895, -0.2290591597557068, 0.32467588782310486, -0.1461770385503769, -0.21636435389518738, -0.02816532552242279, 0.20714549720287323, -0.17578168213367462, -0.3009600341320038, 0.28286585211753845, -0.006994103547185659, -0.12597313523292542, 0.23317451775074005, 0.3803785443305969, 0.3927214741706848, -0.11483442783355713, 0.2829950451850891, 0.018474197015166283, -0.24705329537391663, -0.06750773638486862, -0.3831625282764435, -0.2631472945213318, 0.2048702836036682, 0.479160338640213, 0.12001640349626541, 0.14980991184711456, -0.08352614939212799, 0.0403611920773983, -0.25642991065979004, -0.4158099293708801, 0.05774952098727226, -0.04619700834155083, 0.04551224038004875, 0.24349187314510345, -0.12227887660264969, 0.3542179465293884, -0.21562591195106506, 0.09928877651691437, -0.2353144735097885, -0.3157808482646942, -0.28375840187072754, -0.008973337709903717, 0.17351388931274414, -0.04627449810504913, -0.3089570999145508, -0.0029457630589604378, -0.6132442355155945, -0.33310434222221375, -0.15705206990242004, 0.24114224314689636, 0.4147745370864868, -0.18205879628658295, -0.01971021480858326, -0.19572113454341888, 0.42640140652656555, -0.19011522829532623, 0.10958779603242874, 0.04686036333441734, 0.27683162689208984, -0.030418245121836662, 0.2757188379764557, -0.044967856258153915, 0.042467642575502396, -0.048030950129032135, 0.09655541926622391, 0.16394071280956268, 0.09033085405826569, 0.39237186312675476, -0.2577672004699707, 0.15437686443328857, 0.2673695385456085, 0.5688078999519348, 0.19750255346298218, -0.060536373406648636, 0.20262371003627777, 0.1057218387722969, 0.2705930769443512, -0.38720443844795227, 0.05925577133893967, -0.00688992440700531, 0.10270342975854874, 0.3957236111164093, 0.09983523190021515, 0.3355100452899933, 0.10895264148712158, -0.09636270999908447, -0.012912334874272346, 0.33614590764045715, -0.10542795807123184, 0.2378423511981964, 0.12431690841913223, 0.039020463824272156, 0.13910287618637085, 0.013683010824024677, 0.41979169845581055, -0.01071347389370203, 0.31402644515037537, -0.3290991485118866, 0.24282245337963104, 0.11653146892786026, 0.0871020182967186, -0.29794058203697205, -0.45738160610198975, 0.2896076440811157, 0.02143784798681736, -0.03851159289479256, -0.16954448819160461, -0.2580640912055969, 0.18175210058689117, -0.5244923233985901, -0.08152800798416138, -0.14983898401260376, 0.10359945893287659, -0.06626731902360916, 0.05046974867582321, -0.40117958188056946, -0.3024999499320984, -0.03313251584768295, 0.11974911391735077, 0.08928337693214417, -0.08833255618810654, -0.02614809200167656, 0.024444011971354485, 0.10787694901227951, -0.20801140367984772, 0.12845095992088318, 0.06607743352651596, 0.061891596764326096, -0.03481264412403107, 0.023081829771399498, 0.20329195261001587, 0.06344065070152283, 0.11439638584852219, 0.43032360076904297, 0.33896011114120483, 0.0386466383934021, -0.17889849841594696, 0.17778076231479645, -0.018888937309384346, -0.12378856539726257, -0.09619619697332382, 0.08789597451686859, 0.015653032809495926, 0.16706740856170654, 0.37280336022377014, 0.19511228799819946, -0.21730083227157593, 0.3377448320388794, -0.07064630091190338, -0.13901737332344055, -0.4647118151187897, 0.26698940992355347, 0.1912839710712433, -0.1077180951833725, -0.11948248744010925, -0.2801830470561981, -0.18338261544704437, -0.10936127603054047, 0.3616841733455658, 0.02548488788306713, 0.10699078440666199, -0.18455031514167786, 0.12727071344852448, 0.09304054826498032, 0.42909374833106995, 0.18219269812107086, 0.3078640401363373, -0.28523898124694824, 0.11136015504598618, -0.8087352514266968, -0.15015870332717896, -0.14277705550193787, 0.2350051999092102, -0.052345164120197296, 0.10854530334472656, 0.08479657769203186, 0.43872326612472534, 0.06591378152370453, 0.157931849360466, 0.16264685988426208, -0.18133686482906342, -0.2886347770690918, -0.1446581929922104, -0.08162611722946167, -0.08608977496623993, -0.031425971537828445, -0.3293495774269104, 0.0789925754070282, -0.3852587640285492, 0.02629104070365429, 0.07758219540119171, 0.19740553200244904, -0.20243936777114868, -0.13281701505184174, 0.45995330810546875, 0.024359170347452164, 0.4064575731754303, 0.15641272068023682, -0.11241914331912994, -0.06522446125745773, 0.035319238901138306, -0.057773981243371964, 0.1724693328142166, -0.033867619931697845, -0.026905259117484093, -0.137974813580513, -0.14051306247711182, -0.1662110686302185, 0.20080356299877167, 0.06668911129236221, -0.011972733773291111, -0.11379380524158478, -0.035999760031700134, 0.08679486066102982, 0.14186272025108337, 0.007205980364233255, 0.04317386448383331, -0.14892874658107758, 0.2132108360528946, -0.2026585340499878, 0.04532906040549278, 0.33938276767730713, -0.0350235216319561, 0.012711913324892521, -0.37223339080810547, 0.39237552881240845, 0.16448234021663666, 0.0008880796376615763, -0.4804074764251709, 0.23654049634933472, 0.3327225148677826, -0.15321026742458344, -0.14727875590324402, 0.3244408965110779, -0.25227445363998413, -0.20016321539878845, 0.007166916970163584, 0.25441938638687134, -0.2700773775577545, -0.3051464557647705, 0.06507078558206558, -0.14096786081790924]",
         "22.406654"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>comments</th>\n",
       "      <th>text</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Requiring online connection is a deal breaker ...</td>\n",
       "      <td>Discussion using datasets in offline mode\\n`da...</td>\n",
       "      <td>[-0.47318094968795776, 0.24578382074832916, -0...</td>\n",
       "      <td>25.505016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>The local dataset builders (csv, text , json a...</td>\n",
       "      <td>Discussion using datasets in offline mode\\n`da...</td>\n",
       "      <td>[-0.44908520579338074, 0.2095070332288742, -0....</td>\n",
       "      <td>24.555531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>I opened a PR that allows to reload modules th...</td>\n",
       "      <td>Discussion using datasets in offline mode\\n`da...</td>\n",
       "      <td>[-0.47164809703826904, 0.2902272641658783, -0....</td>\n",
       "      <td>24.148987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>&gt; here is my way to load a dataset offline, bu...</td>\n",
       "      <td>Discussion using datasets in offline mode\\n`da...</td>\n",
       "      <td>[-0.4992601275444031, 0.22699803113937378, -0....</td>\n",
       "      <td>22.894005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Discussion using datasets in offline mode</td>\n",
       "      <td>`datasets.load_dataset(\"csv\", ...)` breaks if ...</td>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>here is my way to load a dataset offline, but ...</td>\n",
       "      <td>Discussion using datasets in offline mode\\n`da...</td>\n",
       "      <td>[-0.49025753140449524, 0.22889599204063416, -0...</td>\n",
       "      <td>22.406654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  \\\n",
       "4  Discussion using datasets in offline mode   \n",
       "3  Discussion using datasets in offline mode   \n",
       "2  Discussion using datasets in offline mode   \n",
       "1  Discussion using datasets in offline mode   \n",
       "0  Discussion using datasets in offline mode   \n",
       "\n",
       "                                                body  \\\n",
       "4  `datasets.load_dataset(\"csv\", ...)` breaks if ...   \n",
       "3  `datasets.load_dataset(\"csv\", ...)` breaks if ...   \n",
       "2  `datasets.load_dataset(\"csv\", ...)` breaks if ...   \n",
       "1  `datasets.load_dataset(\"csv\", ...)` breaks if ...   \n",
       "0  `datasets.load_dataset(\"csv\", ...)` breaks if ...   \n",
       "\n",
       "                                            html_url  \\\n",
       "4  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                            comments  \\\n",
       "4  Requiring online connection is a deal breaker ...   \n",
       "3  The local dataset builders (csv, text , json a...   \n",
       "2  I opened a PR that allows to reload modules th...   \n",
       "1  > here is my way to load a dataset offline, bu...   \n",
       "0  here is my way to load a dataset offline, but ...   \n",
       "\n",
       "                                                text  \\\n",
       "4  Discussion using datasets in offline mode\\n`da...   \n",
       "3  Discussion using datasets in offline mode\\n`da...   \n",
       "2  Discussion using datasets in offline mode\\n`da...   \n",
       "1  Discussion using datasets in offline mode\\n`da...   \n",
       "0  Discussion using datasets in offline mode\\n`da...   \n",
       "\n",
       "                                          embeddings     scores  \n",
       "4  [-0.47318094968795776, 0.24578382074832916, -0...  25.505016  \n",
       "3  [-0.44908520579338074, 0.2095070332288742, -0....  24.555531  \n",
       "2  [-0.47164809703826904, 0.2902272641658783, -0....  24.148987  \n",
       "1  [-0.4992601275444031, 0.22699803113937378, -0....  22.894005  \n",
       "0  [-0.49025753140449524, 0.22889599204063416, -0...  22.406654  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "samples_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
