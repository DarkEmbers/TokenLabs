{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4006417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b6848",
   "metadata": {},
   "source": [
    "### Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbaa90a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(raw_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595d402",
   "metadata": {},
   "source": [
    "### Without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2d79d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) tensor([[-4.0055,  4.3060],\n",
      "        [ 2.2051, -1.8741]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "tensor([[    0.0002,     0.9998],\n",
      "        [    0.9834,     0.0166]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'} \n",
      "\n",
      "POSITIVE\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"Once upon a time lived a majestic fairy king atop the mountains of the world.\",\n",
    "    \"Every universe withholds a corner of darkness.\",\n",
    "]\n",
    "\n",
    "# Tokenize inputs - returns tensor\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass - returns logits\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape, outputs.logits, \"\\n\")\n",
    "\n",
    "# Decode outputs\n",
    "predictions = F.softmax(outputs.logits, dim=-1)\n",
    "print(predictions, \"\\n\")\n",
    "print(model.config.id2label, \"\\n\")\n",
    "\n",
    "for prediction in predictions:\n",
    "\tprint(model.config.id2label[prediction.argmax().item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafcce3",
   "metadata": {},
   "source": [
    "### Load / Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8e83fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# Get pretrained model\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Save to computer\n",
    "model.save_pretrained(\"./Models\")\n",
    "\n",
    "# Load from computer\n",
    "model = AutoModel.from_pretrained(\"./Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8ba57",
   "metadata": {},
   "source": [
    "### Push to / Load from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0036c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f348765b689542a8a02121f9e95a7077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tshar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tshar\\.cache\\huggingface\\hub\\models--DarkEmbers--test-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/DarkEmbers/test-model/commit/140becc969e8c2d6988df09d07cbcdfedc30ec16', commit_message='Upload model', commit_description='', oid='140becc969e8c2d6988df09d07cbcdfedc30ec16', pr_url=None, repo_url=RepoUrl('https://huggingface.co/DarkEmbers/test-model', endpoint='https://huggingface.co', repo_type='model', repo_id='DarkEmbers/test-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"test-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88dc9879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"DarkEmbers/test-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18321f63",
   "metadata": {},
   "source": [
    "### Encode / Decode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36a07b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 1731, 1132, 1128,  136,  102,    0,    0,    0,    0],\n",
      "        [ 101,  146,  112,  182, 2503,  117, 6243, 1128,  106,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# We pad the sentences so they have the same length\n",
    "# Tensors need to be rectangular\n",
    "# Use truncate to shorten sentences that are too long for the model\n",
    "encoded_input = tokenizer(\n",
    "    [\"How are you?\", \"I'm fine, thank you!\"], \n",
    "\tpadding=True, \n",
    "\ttruncation=True,\n",
    "\t# max_length=8,\n",
    "\treturn_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "for key, value in encoded_input.items():\n",
    "\tprint(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48821b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] I ' m fine, thank you! [SEP]\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8524113c",
   "metadata": {},
   "source": [
    "### Tokenizer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ba8e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'you', 'love', 'transform', '##ers', '?']\n",
      "[1790, 112, 189, 1128, 1567, 11303, 1468, 136]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Don ' t you love transformers?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokens = tokenizer.tokenize(\"Don't you love transformers?\")\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ad06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Feeding tokenizer ids to model\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "outputs = model(input_ids)\n",
    "# Softmax to get probabilities\n",
    "preds = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "for prediction in preds:\n",
    "\tprint(model.config.id2label[prediction.argmax().item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
