{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e53d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85e5687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319e723",
   "metadata": {},
   "source": [
    "## Single Step Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff40834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batch\n",
    "sequences = [\n",
    "\t\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "\t\"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Add Lables\n",
    "batch[\"labels\"] = torch.tensor([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on single batch\n",
    "optimizer = optim.AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e371e39",
   "metadata": {},
   "source": [
    "## Paraphrase Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67997c74",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80a954ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 3668\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 408\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "         num_rows: 1725\n",
       "     })\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "     num_rows: 3668\n",
       " }),\n",
       " {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       "  'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       "  'label': 1,\n",
       "  'idx': 0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "raw_datasets, raw_datasets[\"train\"], raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00c3fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value('string'),\n",
       " 'sentence2': Value('string'),\n",
       " 'label': ClassLabel(names=['not_equivalent', 'equivalent']),\n",
       " 'idx': Value('int32')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See labels\n",
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c3353c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Convert sentences to tokens\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentence1 = tokenizer(raw_datasets[\"train\"][0][\"sentence1\"])\n",
    "tokenized_sentence2 = tokenizer(raw_datasets[\"train\"][0][\"sentence2\"])\n",
    "\n",
    "print(tokenized_sentence1)\n",
    "print(tokenized_sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b136aaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5252431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]]),\n",
      " 'input_ids': tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
      "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
      "          3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
      "          1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
      "          2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Pair the sentences to feed into the model\n",
    "# Notice token_type_ids has 0 and 1, indicating which sentence it is\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"sentence1\"], raw_datasets[\"train\"][0][\"sentence2\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "pprint(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc91e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '\"', 'the', 'witness', '\"', ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '\"', 'the', 'witness', '\"', ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Decode each sentence\n",
    "# Notice the [SEP] token which tells us where the second sentence starts\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bb348",
   "metadata": {},
   "source": [
    "### Full Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b401dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Function to tokenize and pair sentences\n",
    "def tokenize_function(example):\n",
    "\treturn tokenizer(\n",
    "    \texample[\"sentence1\"],\n",
    "     \texample[\"sentence2\"],\n",
    "       \ttruncation=True\n",
    "    )\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "# Tokenize all data\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# Collate function is used to make batches\n",
    "# We need dynamic padding, so we define a collate function\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee933b9",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d9253",
   "metadata": {},
   "source": [
    "#### Using Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 9:32:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.357098</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.902027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>0.554072</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.892562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.704931</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.32329054024224707, metrics={'train_runtime': 34360.0159, 'train_samples_per_second': 0.32, 'train_steps_per_second': 0.04, 'total_flos': 405114969714960.0, 'train_loss': 0.32329054024224707, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Now training with compute_metrics\n",
    "# Now we get val loss and metrics (i.e. accuracy and f1)\n",
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\", fp16=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Since processing_class is tokenizer, data_collator is set to DataCollatorWithPadding\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator, # Added to show importance\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7754d35",
   "metadata": {},
   "source": [
    "#### Using Raw Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec06802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing this again so doesn't fail when re-running\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") # Becuase pytorch expects labels\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2171b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': torch.Size([8, 72]),\n",
      " 'input_ids': torch.Size([8, 72]),\n",
      " 'labels': torch.Size([8]),\n",
      " 'token_type_ids': torch.Size([8, 72])}\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "\tpprint({k: v.shape for k, v in batch.items()})\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e26842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e8646a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "\t\"linear\",\n",
    "\toptimizer=optimizer,\n",
    "\tnum_warmup_steps=0,\n",
    "\tnum_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "num_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d440073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef0ae08ee2c46d899fa9bebd3932551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8504901960784313, 'f1': 0.8939130434782608}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "\tfor batch in train_dataloader:\n",
    "\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
    "\t\toutputs = model(**batch)\n",
    "\t\tloss = outputs.loss\n",
    "\t\tloss.backward()\n",
    "\t\t\n",
    "\t\toptimizer.step()\n",
    "\t\tlr_scheduler.step()\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tprogress_bar.update(1)\n",
    "  \n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in val_dataloader:\n",
    "\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(**batch)\n",
    "\t\tpredictions = outputs.logits.argmax(dim=-1)\n",
    "\t\tmetric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "print(metric.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
